\section{Verifying Demand with Converging Evidence}\label{sec:verification}

This section outlines operational strategies for validating experimentally induced cognitive load. Recent studies have begun using task-complexity manipulations to investigate the effects of cognitive load on L2 performance \parencite[e.g.,][]{Bergeron2017,crowther2018,fullana2024,SaitoLiu2022}. Nevertheless, as \textcite{sasayama2016} notes, designed differences between tasks of varying complexity do not always translate into actual cognitive differences. The present thesis therefore treats verification of cognitive load as a prerequisite for interpretation and, in each empirical strand, adopts a theory-aligned set of subjective and objective indicators to document that demand was actually elicited.

\subsection{Categories of Cognitive Load Measures}

Techniques for measuring task-induced cognitive load have been developed mainly in cognitive psychology and multimedia learning. A common scheme classifies measures by objectivity and causal proximity, yielding four types: subjective or objective, and direct or indirect \parencite{Brunken2003}.

First, subjective–indirect measures record invested mental effort. A standard approach asks participants to rate perceived effort on a nine-point scale from very low to very high immediately after a task \parencite{paas1994_1}. These ratings are efficient and sensitive to momentary strain; however, they require corroboration because a low reported effort score can reflect genuinely low demand or, conversely, a strategic minimization in the face of high demand \parencite{Brunken2003,vanGogPaas2008}.

Second, subjective–direct measures target perceived difficulty or stress explicitly attributed to the materials or delivery format. For example, participants can judge, on a graded scale, how easy or difficult computer-based instructions were to understand \parencite{Kalyuga1999}. The intended advantage is a closer linkage to the load imposed by the materials.

Third, objective–indirect measures register behavioral or physiological consequences of demand. Behavioral indices include time on task and error patterns, which typically covary with increases in cognitive complexity \parencite{Brunken2003}. Physiological indices include cardiac activity, pupil dilation (mean level and dilation range), blink rate, eye movements and fixations, and skin conductance. These measures avoid subjective bias, yet they remain indirect because multiple factors can intervene between cognitive state and observed behavior \parencite{Brunken2003}.

Fourth, objective–direct measures aim to index capacity sharing itself. Neuroimaging can, in principle, reveal task-related activation; however, links between such activation and cognitive load during elaborated L2 communication remain limited, and practical constraints are substantial \parencite{Brunken2003}. For applied settings, a dual-task paradigm is typically recommended. A simple secondary probe draws on the same general pool of attention; when primary-task demand rises, responses to the probe slow or become less accurate \parencite{brunken2002}. Probe design matters for sensitivity: if the secondary task is too easy (e.g., background-color change), participants can maintain performance regardless of primary-task load, but when selection demands are higher (e.g., detecting a letter-color change in a defined region), differences between presentation modes and task versions are detected more reliably \parencite{Schoor2012}.

Within task-based language teaching, these four families have been adapted to validate designed task complexity independently of linguistic performance. Self-rating questionnaires differentiate nominally simple from complex tasks, yet they primarily index perceived difficulty rather than the intended design complexity \parencite{Robinson2001}. Introspective methods, including stimulated recall and interviews, can demonstrate whether the more complex version actually recruited the intended reasoning and comparison operations even when perceived difficulty is similar \parencite{Kim_Payant_Pearson_2015}. Retrospective time-estimation tasks also distinguish complex from simple versions in the predicted direction: under higher reasoning demands, participants tend to overestimate elapsed time relative to the actual duration \parencite{Baralt_2013}. Eye tracking further shows that higher reasoning demands elicit more and longer fixations than simpler counterparts; in the same studies, dual-task probes yield slower and less accurate responses under the more complex condition, providing a capacity-based check on the subjective and introspective evidence \parencite{revesz2014}. Across categories, triangulation of at least two independent indices remains advisable for interpretability and for ruling out strategic responding \parencite{Brunken2003,vanGogPaas2008}.

\subsection{Boundary Conditions and Validation Practices}

A recurring challenge is ensuring that designed task differences actually map onto cognitive load in practice \parencite{revesz2014}. In studies linking listener judgments to linguistic dimensions, researchers frequently validate task complexity with participants’ self-ratings of difficulty or effort \parencite[e.g.,][]{crowther2018,fullana2024}. Such subjective checks are informative but, on their own, not sufficient.

Evidence from capacity-based validations points to two boundary conditions. First, only relatively large increases in element interactivity reliably register across dual-task, time-estimation, and self-rating measures: in monologic speaking, one- vs.\ nine-element tasks diverged robustly, whereas intermediate steps were less consistently distinguished \parencite{sasayama2016}. Second, subjective and objective indices can differ in apparent magnitude. Self-ratings often yield larger effects than dual-task or reaction-time-based measures, plausibly because perceptions can be swayed by salient but shallow features, whereas objective probes can miss subtler load if the secondary task is under-calibrated or if participants strategically disengage \parencite{sasayama2016}. The practical implication is to combine streams and to design secondary probes with sufficient selection demands and irregular timing, while avoiding sensory interference with the primary task \parencite{Schoor2012,brunken2002}.

These boundary conditions motivate the converging-evidence approach adopted throughout this thesis: subjective effort ratings are paired with secondary-task performance in the production studies (Studies 1--2) and with pupillometric indices in the comprehension studies (Studies 3--5).



%%%%%%%%%%
\begin{quote}
    \textbf{CAUTION:} REMOVE BELOW? The above para already covers this.


\subsection{Operationalization and Verification in This Thesis}

\textbf{Production (Studies~1--2).} After each trial, participants provide a single-item effort rating on the nine-point scale \parencite{paas1994_1,paas2003}. During speaking, a sparse and unpredictable secondary task samples spare capacity and is summarized as a speed-accuracy composite \parencite{vandierendonck2017}. The probe is practiced before measurement; timing is irregular to discourage strategic scheduling; sensory demands are chosen to avoid conflict with speech processing \parencite{brunken2002,Schoor2012}. Verification rule: effort ratings must increase in the predicted condition and the secondary-task composite must tighten in the same direction; production outcomes are interpreted only after demand has been verified \parencite{KormosDenes2004,revesz2014}.

\textbf{Comprehension (Studies~3--5).} All three comprehension studies draw on Experiment~2 but analyze different evidence streams. After each passage section, participants report experienced effort on a single-item scale; comprehension accuracy is assessed after learning.

Study~3 analyzes subjective cognitive load and self-efficacy ratings together with comprehension accuracy; eye movements are not analyzed in this study.

Study~4 analyzes eye movements in the text modality. Early measures (first-fixation duration, gaze duration) index recognition and initial lexical access. Late measures (total fixation time, regression count) index integration and discourse-level processing. Spatial indices (e.g., Radius of gyration and Convex hull area) capture the spatial dispersion of fixations to quantify how reading patterns are organized on the page.

Study~5 models the complete pupil-diameter trajectory within each passage section across both text and video modalities, providing a time-resolved physiological index of moment-to-moment effort beyond aggregate eye-movement measures. 

Analyses proceed only when demand has been verified: effort ratings must increase in the intended condition and at least one ocular index must move in the predicted direction, with interpretations kept proportionate where streams diverge \parencite{sasayama2016,Lee2019}.
\end{quote}