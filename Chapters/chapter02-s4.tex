\section{Measurement and Validation of Cognitive Load}\label{sec:verification}

Having established how cognitive load can be manipulated through intrinsic and extraneous design features (\autoref{sec:manipulations}), this section outlines operational strategies for validating that these manipulations actually produce the intended increases in demand.
This section outlines operational strategies for validating experimentally induced cognitive load. Recent studies have begun using task-complexity manipulations to investigate the effects of cognitive load on L2 performance \parencite[e.g.,][]{Bergeron2017,crowther2018,fullana2024,SaitoLiu2022}. Nevertheless, as \textcite{sasayama2016} notes, designed differences between tasks of varying complexity do not always translate into actual cognitive differences. The present thesis therefore treats verification of cognitive load as a prerequisite for interpretation and, in each empirical strand, adopts a theory-aligned set of subjective and objective indicators to document that demand was actually elicited.

\subsection{Categories of Cognitive Load Measures}

Techniques for measuring task-induced cognitive load have been developed mainly in cognitive psychology and multimedia learning. A common scheme classifies measures by objectivity and causal proximity, yielding four types: subjective or objective, and direct or indirect \parencite{Brunken2003}.

First, subjective–indirect measures record invested mental effort. A standard approach asks participants to rate perceived effort on a nine-point scale from very low to very high immediately after a task \parencite{paas1994_1}. These ratings are efficient and sensitive to momentary strain; however, they require corroboration because a low reported effort score can reflect genuinely low demand or, conversely, a strategic minimization in the face of high demand \parencite{Brunken2003,vanGogPaas2008}.

Second, subjective–direct measures target perceived difficulty or stress explicitly attributed to the materials or delivery format. For example, participants can judge, on a graded scale, how easy or difficult computer-based instructions were to understand \parencite{Kalyuga1999}. The intended advantage is a closer linkage to the load imposed by the materials.

Third, objective–indirect measures register behavioral or physiological consequences of demand. Behavioral indices include time on task and error patterns, which typically covary with increases in cognitive complexity \parencite{Brunken2003}. Physiological indices include cardiac activity, pupil dilation (mean level and dilation range), blink rate, eye movements and fixations, and skin conductance. These measures avoid subjective bias, yet they remain indirect because multiple factors can intervene between cognitive state and observed behavior \parencite{Brunken2003}.

Fourth, objective–direct measures aim to index capacity sharing itself. Neuroimaging can, in principle, reveal task-related activation; however, links between such activation and cognitive load during elaborated L2 communication remain limited, and practical constraints are substantial \parencite{Brunken2003}. For applied settings, a dual-task paradigm is typically recommended. A simple secondary probe draws on the same general pool of attention; when primary-task demand rises, responses to the probe slow or become less accurate \parencite{brunken2002}. Probe design matters for sensitivity: if the secondary task is too easy (e.g., background-color change), participants can maintain performance regardless of primary-task load, but when selection demands are higher (e.g., detecting a letter-color change in a defined region), differences between presentation modes and task versions are detected more reliably \parencite{Schoor2012}.

Within task-based language teaching, these four families have been adapted to validate designed task complexity independently of linguistic performance. Self-rating questionnaires differentiate nominally simple from complex tasks, yet they primarily index perceived difficulty rather than the intended design complexity \parencite{Robinson2001}. Introspective methods, including stimulated recall and interviews, can demonstrate whether the more complex version actually recruited the intended reasoning and comparison operations even when perceived difficulty is similar \parencite{Kim_Payant_Pearson_2015}. Retrospective time-estimation tasks also distinguish complex from simple versions in the predicted direction: under higher reasoning demands, participants tend to overestimate elapsed time relative to the actual duration \parencite{Baralt_2013}. Eye tracking further shows that higher reasoning demands elicit more and longer fixations than simpler counterparts; in the same studies, dual-task probes yield slower and less accurate responses under the more complex condition, providing a capacity-based check on the subjective and introspective evidence \parencite{revesz2014}. 

\subsection{Requirements for Reliable Validation}
A recurring challenge is ensuring that designed task differences actually map onto cognitive load in practice \parencite{revesz2014}. Subjective self-ratings of difficulty or effort are informative but, on their own, insufficient for validating manipulations \parencite{crowther2018,fullana2024}.

Evidence from capacity-based validations reveals two critical boundary conditions:

First, manipulations must be large enough to register. Only substantial increases in element interactivity reliably produce measurable effects across dual-task, time-estimation, and self-rating measures. For example, in monologic speaking tasks, contrasts between one-element and nine-element tasks showed robust differences, whereas intermediate steps (e.g., three versus five elements) were inconsistently distinguished \parencite{sasayama2016}.

Second, subjective and objective indices can diverge. Self-ratings often show larger effects than dual-task or reaction-time measures. This divergence likely occurs because subjective ratings can be influenced by surface-level task features (e.g., visual clutter, time pressure), whereas objective probes may fail to detect genuine load if the secondary task is poorly calibrated or if participants disengage strategically \parencite{sasayama2016}.

The practical implication is straightforward: validation requires multiple, independent measures. Secondary probes should have sufficient attentional demands and irregular timing to prevent strategic anticipation, while avoiding sensory overlap with the primary task \parencite{Schoor2012,brunken2002}.

These principles motivate the converging-evidence approach throughout this thesis: subjective effort ratings are paired with secondary-task performance in the production studies (Studies 1--2) and with pupillometric indices in the comprehension studies (Studies 3--5).

