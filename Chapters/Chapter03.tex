%************************************************
\chapter{Experiment 1 Methods (Studies 1 and 2)}\label{ch:exp1-methods}
%************************************************


\section{Introduction}


Experiment 1 investigates L2 speaking performance under controlled intrinsic load by manipulating element interactivity (\autoref{subsec:op-production}). The experiment addresses two questions. Study 1 asks: Which dimensions of complexity, accuracy, and fluency do learners protect under validated load, which do they allow to give way, and under what conditions are concurrent gains possible? Study 2 asks: Which linguistic dimensions best predict comprehensibility and perceived fluency for native listeners, and does the relative weight of those predictors shift with task demand?

To address these questions, speakers perform monologic argumentative tasks under low-complexity and high-complexity conditions. Audio recordings from these tasks enable Study 1 (\autoref{ch:study1}) to quantify how speakers reallocate attention among complexity, accuracy, and fluency. Native listener ratings of the same recordings enable Study 2 (\autoref{ch:study2}) to estimate the relative contributions of linguistic dimensions to comprehensibility and perceived fluency judgments and to test whether those contributions vary with task demand.

\section{Participants}


\subsection{Speakers}
36 Chinese-speaking learners of Japanese (24 females; ages: 22--30, $M=25.08$, $SD=1.87$) participated voluntarily. All were university students in Japan during the experiment. All had passed the Japanese-Language Proficiency Test (JLPT) at either the N1 (highest level; $n=27$) or N2 (second-highest level; $n=9$) level. However, JLPT uses a pass/fail system at each proficiency level; it cannot provide fine distinctions or rankings within any single level \parencite{nishizawa2022}. Therefore, we also administered the web version of Simple Performance-Oriented Test (SPOT90; \citeauthor{kobayashi2015}, \citeyear{kobayashi2015}). SPOT90 has established use in Japanese language education and reported reliability \parencite{kobayashi2015}. Its tasks required participants to view one sentence with a blank while hearing the full sentence, then select one hiragana from four options. Speakers' average SPOT90 score was 71.49 ($SD=8.48$; range=$42$--$88$), reflecting mostly intermediate (56--80) to advanced (81--90) proficiency. One speaker was excluded from the linguistic data analysis due to technical recording issues, so the final sample for the main linguistic analyses was $N=35$. However, this speaker's data was used to validate the manipulation of task complexity. All participants provided informed consent, and the study was approved by the Human Subjects Research Ethics Review Committee of the Institute of Science Tokyo (Approval No. 2019171).

\subsection{Raters}
To assess comprehensibility and perceived fluency for Study 2 (\autoref{ch:study2}), eight native Japanese raters evaluated the samples. All were born and raised in Japan in Japanese-speaking homes, and each primarily speaks the Tokyo dialect. Recruiting only Tokyo-dialect speakers aimed to minimize dialectal perceptual variance \parencite{kubozono2012}, particularly regarding the influence of pitch accent on lexical access \parencite{cutler1999}. At the time of the study, all were employed as university-level Japanese language instructors. Their mean teaching experience was 22.88 years ($SD = 9.60$). They reported high familiarity (1 = \textit{not at all}, 7 = \textit{very familiar}) with foreign-accented ($M = 6.88$, $SD = 0.35$) and Chinese-accented Japanese ($M = 6.38$, $SD = 0.92$). They also reported frequent contact (1 = \textit{never}, 7 = \textit{very frequently}) with Japanese language learners overall ($M = 6.88$, $SD = 0.35$) and specifically those from China ($M = 6.00$, $SD = 1.41$).


\section{Materials and Tasks}

\subsection{Primary Oral Task}\label{subsec:primary-task}
This study manipulated $\pm$few elements for its intuitive appeal and ease of implementation to operationalize task complexity. The argumentative task was adapted from \parencite{michel2011} for a Japanese context, with two versions differing in complexity: a low-complexity task (+few elements) and a high-complexity task ($-$few elements). In the low-complexity version, each speaker saw four photographs of young adults on a PC screen; in the high-complexity version, they saw eight. Alongside each photograph were four attributes: job, personality, favorite hobby, and ``type'' (in the sense of a romantic partner). The instructions described these individuals, balanced by gender, as dating-show contestants. Speakers were required to form two male-female couples in the low-complexity task (choosing from four contestants) or four couples in the high-complexity task (from eight). The expected speech duration was 90 seconds for the low-complexity task and 150 seconds for the high-complexity task.





\subsection{Secondary task}
To assess the cognitive load of the primary oral task, speakers performed a secondary reaction task concurrently. Because attentional resources are limited, higher demands on the primary task should lead to slower and less accurate responses in the secondary task \parencite{brunken2002}. The secondary task was spatially configured to minimize interference with the primary speaking task while remaining within the participant's peripheral vision. The speech task stimuli were displayed in a central region with a resolution of 1520 $\times$ 900 pixels, while the secondary task occupied the area between this central region and the edges of the monitor (resolution: 1920 $\times$ 1080 pixels), as illustrated in Figure \ref{fig:dual-task-layout}. During speaking, the background color in this peripheral region randomly changed from white to blue or red at intervals ranging from 500 to 3,000~ms. Participants were instructed to press the left arrow key for white-to-blue transitions and the right arrow key for white-to-red transitions as quickly and accurately as possible. 

Both tasks were administered through PsychoPy3 (Version 3.2.4; \citeauthor{peirce2019}, \citeyear{peirce2019}), which recorded error rates and reaction times. The linear integrated speed-accuracy score (LISAS; \citeauthor{vandierendonck2017}, \citeyear{vandierendonck2017}) was then calculated to capture both speed and accuracy in a single measure.

\begin{figure}[htbp]
\centering
\includegraphics[width=\textwidth]{Chapters/c3fig/dual_task_layout.pdf}
\caption{Spatial configuration of the dual-task display. The primary speech task stimuli were presented in the central region (1520~×~900 pixels, white area with dashed border), while the secondary reaction task involved responding to color changes (white to blue or red) occurring in the peripheral region (shaded area) between the primary task area and the monitor edges (1920$ \times$ 1080 pixels).}
\label{fig:dual-task-layout}
\end{figure}


\subsection{Questionnaires for Subjective Task Difficulty and Mental Effort}

Following the approach of \textcite{sasayama2016}, this study employed self-assessment scales for perceived task difficulty and mental effort to capture speakers' subjective experiences with the tasks. The perceived difficulty scale, as used by \textcite{Robinson2001}, allowed participants to rate how difficult they found each task, providing insight into their subjective perception of task demands (1 = \textit{Extremely easy}, 9 = \textit{Extremely difficult}). The mental effort scale, adapted from \parencite{paas1994_1}, was used to assess the amount of cognitive resources participants believed they allocated to each task (1 = \textit{Extremely low}, 9 = \textit{Extremely high}). All questions in these scales were presented in Japanese.




\section{Procedure}

\subsection{Procedure of Oral Task}

The experiment lasted approximately one hour per speaker, conducted individually in a quiet room with a within-subject design. Each speaker performed low- and high-complexity oral tasks in random order. After reading task instructions in their first language (Chinese), they completed a practice task, structured like the low-complexity task but conducted in Chinese, to familiarize themselves with the procedure. The speakers were given 90 seconds to review the materials before orally presenting their matching results and reasoning in Japanese. After completing each task, they filled out the self-assessment questionnaire of perceived difficulty and mental effort. Finally, they took the SPOT to assess their Japanese proficiency. All responses were recorded for analysis.

\subsection{Procedure of Rating Task}



The eight raters evaluated the speech samples using a 9-point scale (Comprehensibility: 1 = \textit{hard to understand}, 9 = \textit{easy to understand}; Perceived Fluency: 1 = \textit{not fluent at all}, 9 = \textit{very fluent}), following \parencite{SuzukiKormos2020}. Before the main rating task, raters underwent a training session (approximately one hour) led by the authors. This session involved: (1) a brief explanation of comprehensibility and perceived fluency to establish a shared conceptual understanding for intuitive judgments; (2) an introduction to the 9-point scale and its usage; (3) presentation of the speaking task prompt to the raters; (4) practice rating the eight sample recordings (not used in the main rating procedure) to calibrate their understanding and promote rating consistency. During the training, comprehensibility was described to the raters as ``how easy or difficult it is for a listener to understand a speaker's utterance,'' and perceived fluency was described as ``the overall smoothness and ease of the speaker's delivery.'' Raters evaluated speech samples for both constructs simultaneously using a customized web-based application developed by the authors at their own pace. The rating session, including breaks, lasted approximately three hours, and the sample order was counterbalanced. Recordings were trimmed to the first 60 seconds, excluding initial fillers and dysfluencies. If a sentence was ongoing at 60 seconds, it continued until completion; otherwise, trimming stopped at the end of the last complete sentence within 60 seconds. Samples under 60 seconds were used in full. Mean lengths were 55.60 seconds ($SD = 13.31$, range = 22.15--82.39~s) for the low-complexity task and 60.37 seconds ($SD = 11.10$, range = 22.00--79.67~s) for the high-complexity task.


\section{Apparatus and Recording}

\section{Transcription and Coding Procedures}

Following \citeauthor{SuzukiKormos2020} (\citeyear{SuzukiKormos2020}), we used objective measures for the linguistic dimensions. Speech samples were transcribed verbatim, excluding filled pauses, repetitions, false starts, and self-corrections, and then segmented into Analysis of Speech units (AS-units; \citeauthor{foster2000}, \citeyear{foster2000}) and clauses. Although T-units \parencite{hunt1965} are also commonly used, we opted for AS-units because they better handle the fragmented nature of spoken data \parencite{foster2000}. Both AS-units and clauses were coded using \citeauthor{sakuragi2011}'s (\citeyear{sakuragi2011}) scheme, which adapts \citeauthor{foster2000}'s (\citeyear{foster2000}) AS-unit definition for Japanese, considering clauses with the same subject as one unit, and different subjects as separate units. Clauses were identified at each predicate. 



For instance, the utterance \textbar{}[Because Hanako \underline{came}]\textbar{}, \textbar{}[Taro \underline{boiled} water][and \underline{made} tea.]\textbar{} (Japanese: 
\textbar{}[\begin{CJK}{UTF8}{ipxm}花子が\underline{来た}ので]\textbar{}, \textbar{}[太郎は湯を\underline{沸かして}][お茶を\underline{いれた}\end{CJK}.]
\textbar{}) was segmented into two AS-units (demarcated by \textbar{}) and contains three clauses (identified by underlined predicates), where the boundaries of each clause are indicated by the square brackets ([]). 

The author and an independent coder performed segmentation separately. Next, the thesis supervisor and the independent coder separately identified lexical, morphological, and syntactic errors. The supervisor resolved all disagreements through reanalysis. For lexical complexity, we identified content and function words using MeCab \parencite{kudo2004}, a morphological analyzer for Japanese text, paired with the NEologd dictionary \parencite{sato2015} for broad coverage of neologisms. MeCab was accessed through RMeCab \parencite{ishida2022}. From the RMeCab output, content words were tagged as independent verbs (\textit{jiritsudoshi}), nouns (\textit{meishi}), adjectives (\textit{keiyoshi}), adjectival nouns (\textit{keiyodoshi}), adverbs (\textit{fukushi}), and adnominals (\textit{rentaishi}). To ensure only core lexical items, suffixes (\textit{setsubi}) were excluded. Function words were categorized as case particles (\textit{kakujoshi}), conjunctive particles (\textit{setsuzokujoshi}), adverbial particles (\textit{fukujoshi}), and auxiliary verbs (\textit{jodoshi}). For fluency annotation, we first used an automated silence detection script \parencite{dejong2009} to detect pauses longer than 250~ms \parencite{bosker2013}. We then manually coded AS-unit boundaries and refined these pause boundaries using Praat \parencite{boersma2001}.

\section{Linguistic Dimensions}

Based on the coded data described above, the specific linguistic dimensions analyzed in this study are listed below.


\begin{flushleft}
\textsc{Complexity}

\textit{Syntactic complexity}
\begin{enumerate}
    \item Clauses per AS-unit: The mean number of clauses produced per AS-unit.
    \item Case particles per AS-unit: The mean number of case particles produced per AS-unit.
    \item Conjunctive particles per AS-unit: The mean number of conjunctive particles produced per AS-unit.
    \item Adverbial particles per AS-unit: The mean number of adverbial particles produced per AS-unit.
    \item Auxiliary verbs per AS-unit: The mean number of auxiliary verbs produced per AS-unit.
\end{enumerate}

\textit{Lexical complexity}
\begin{enumerate}
    \item Lexical density: The ratio of content words to the total words.
    \item Different content words per AS-unit: The mean number of unique content words produced per AS-unit.
    \item Number of content words per AS-unit: The mean number of content words produced per AS-unit.
\end{enumerate}

\textsc{Accuracy}
\begin{enumerate}
    \item Error rate: The mean number of errors (lexical, morphological, and syntactic) per AS-unit.
\end{enumerate}

\textsc{Fluency}

\textit{Speed fluency}
\begin{enumerate}
    \item Speech rate: The mean number of moras per second, calculated by dividing the total number of moras by the total speech duration.
    \item Articulation rate: The mean number of moras per second, calculated by dividing the total number of moras by the total phonation time (i.e., total speech duration, excluding pauses).
    \item Mean length of run: The average number of moras per run, where a run is defined as a sequence of moras without a pause (silent or filled) longer than 250~ms.
\end{enumerate}

\textit{Breakdown fluency}
\begin{enumerate}
    \item Mid-clause pause duration: The average length of unfilled pauses occurring within clauses, measured in seconds.
    \item Mid-clause pause ratio: The ratio of unfilled pauses within clauses to total words.
    \item Final-clause pause duration: The average length of unfilled pauses occurring between clauses, measured in seconds.
    \item Final-clause pause ratio: The ratio of unfilled pauses between clauses to total words.
    \item Filled pause ratio: The ratio of filled pauses to total words.
\end{enumerate}

\textit{Repair fluency}
\begin{enumerate}
    \item Dysfluency rate: The ratio of dysfluencies (false starts, repetitions, reformulations, and self-corrections) to total speech duration in seconds.
\end{enumerate}
\end{flushleft}



\subsection{Validation of Task Complexity Manipulation}

To confirm the successful manipulation of task complexity, we analyzed speakers' self-reported perceived difficulty and mental effort, and their performance on the secondary task (LISAS). Table \ref{tab:task_validation_study2} presents the descriptive statistics for these measures. A Wilcoxon signed-rank test showed that speakers rated the high-complexity task as significantly more difficult ($M = 7.22$, $SD = 1.57$) than the low-complexity task ($M = 5.44$, $SD = 1.52$), $p < .001$, with a mean difference of $1.78$, $95\%$ CI $[1.33, 2.22]$, and a large effect size ($r = 0.94$). They also reported that the high-complexity task required significantly more mental effort ($M = 7.31$, $SD = 1.33$) than the low-complexity task ($M = 5.81$, $SD = 1.37$), $p < .001$, with a mean difference of $1.50$, $95\%$ CI $[1.10, 1.90]$, and a similarly large effect size ($r = 0.93$). A paired-samples \textit{t}-test indicated that LISAS scores were higher in the high-complexity task ($M = 2189.57$, $SD = 501.66$) compared to the low-complexity task ($M = 2044.54$, $SD = 457.11$), $t(35) = 3.00$, $p = .013$, with a mean difference of $145.02$, $95\%$ CI $[32.70, 257.35]$, the effect size was smaller than small ($d = 0.44$). These findings confirm that the high-complexity task elicited greater perceived difficulty, demanded more mental effort, and produced longer reaction times than the low-complexity task, thereby validating the intended cognitive complexity manipulation.



\begin{table}[ht]
\centering

\begin{threeparttable}
\caption{Task difficulty, mental effort, and LISAS scores}
\footnotesize
\label{tab:task_validation_study2}
\begin{tabular}{lSSSS}
\toprule
& & & \multicolumn{2}{c}{95\% CI} \\
\cline{4-5}
Measures & \textit{M} & \textit{SD} & \textit{Lower} & \textit{Upper} \\
\midrule
\multicolumn{5}{l}{\textsc{Low-complexity task}} \\
Perceived difficulty & 5.44 & 1.52 & 4.93 & 5.96 \\
Mental Effort & 5.81 & 1.37 & 5.34 & 6.27 \\
LISAS & 2044.54 & 457.11 & 1890.00 & 2190.00 \\
Accuracy Rate (\%) & 78.05 & 17.29 & 72.11 & 83.99 \\
RTCorrect & 1659.73 & 723.07 & 1565.14 & 1754.33 \\
\midrule
\multicolumn{5}{l}{\textsc{High-complexity task}} \\
Perceived difficulty & 7.22 & 1.57 & 6.69 & 7.75 \\
Mental Effort & 7.31 & 1.33 & 6.86 & 7.75 \\
LISAS & 2189.57 & 501.66 & 2020.00 & 2359.00 \\
Accuracy Rate (\%) & 73.84 & 20.42 & 66.82 & 80.85 \\
RTCorrect & 1729.32 & 734.43 & 1639.30 & 1819.33 \\
\bottomrule
\end{tabular}
\begin{tablenotes}
\footnotesize
\item \textit{Note.} RTCorrect = reaction time for correct responses (in milliseconds)
\end{tablenotes}
\end{threeparttable}
\end{table}
