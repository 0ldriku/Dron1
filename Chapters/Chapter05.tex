%************************************************
\chapter{Study 2: Listener Judgments Under Load}\label{ch:study2}
%************************************************

\section{Introduction}



Study 1 (\autoref{ch:study1}) established that L2 speakers' production profiles are systematically reshaped by cognitive load. However, effective L2 communication depends not only on what speakers produce but also on how listeners perceive and interpret that output \parencite{baeseberk2020}. This leads to the central question for the present study: What are the perceptual consequences of these production trade-offs for native listeners? Study 2 addresses this gap by examining how the shifts in speakers' production influence native listeners' judgments. We focus on two essential components of successful L2 communication: comprehensibility (the listeners' perception of how easily they understand L2 speech; \citeauthor{MunroDerwing1999}, \citeyear{MunroDerwing1999}) and perceived fluency (listeners' inferences about underlying cognitive fluency based on observable temporal features; \citeauthor{Segalowitz2010}, \citeyear{Segalowitz2010}). Both constructs are crucial for real-world interaction \parencite{DerwingMunro2009} and are assessed in high-stakes tests such as the International English Language Testing System (IELTS) and the Test of English as a Foreign Language (TOEFL) \parencite{IsaacsTrofimovich2012}. 

The central theoretical question is whether listeners rely on a stable set of linguistic dimensions when judging L2 speech, or whether the salience of different dimensions changes as speaker production is compromised under cognitive load. To test these predictions, this study examines three complementary questions. First, we assess the empirical overlap between comprehensibility and perceived fluency across task conditions to determine whether they remain distinct constructs under load (RQ1). Second, we model which linguistic dimensions predict comprehensibility in low- versus high-complexity tasks to test whether listeners broaden their criteria under load (RQ2). Third, we conduct parallel analyses for perceived fluency to determine whether its linguistic correlates remain more stable across task demands (RQ3). Together, these analyses provide a listener-centered account of the communicative consequences of the production trade-offs documented in Study 1, clarifying which aspects of load-compromised speech most strongly shape native listeners' perceptions.





\section{Background}

\subsection{Linguistic Dimensions of Comprehensibility and Perceived Fluency}

\subsubsection{Comprehensibility}
Extensive research in second language acquisition has explored the linguistic dimensions that underlie comprehensibility. At the segmental level, errors in high-functional-load consonants significantly impair comprehensibility \parencite{MunroDerwing2006}. At the suprasegmental level, prosodic features, such as sentence stress \parencite{hahn2004}, word stress \parencite{IsaacsTrofimovich2012}, and pitch range and pausing \parencite{kang2010} are linked to comprehensibility. As for lexical and grammatical aspects, studies indicate that low grammatical accuracy \parencite{MunroDerwing1995} and poor word choice \parencite{fayer1987} can negatively affect comprehensibility. More recently, \textcite{saito2016a} have highlighted that comprehensibility in L2 English is linked to phonology, grammar, vocabulary, and discourse. However, more recent insights from \textcite{SuzukiKormos2020} suggest that the specific linguistic dimensions that primarily drive listeners' perceptions of comprehensibility remain unclear, pointing to the need for further research to disentangle these complex relationships.

\subsubsection{Perceived Fluency}
As for perceived fluency, researchers have investigated the relationship between utterance fluency and perceived fluency to identify the specific temporal features that influence listeners' judgments of fluency. There are mixed findings regarding the relative importance of speed fluency (e.g., speech rate) and breakdown fluency (e.g., pause) related to perceived fluency. Some studies find that speed fluency measures correlate more strongly with perceived fluency \parencite[e.g.,][]{bosker2013}, while others show that breakdown fluency measures are more strongly associated with perceived fluency \parencite{cucchiarini2002,SuzukiKormos2020}. Taken together, the relationship between utterance fluency and perceived fluency has similar patterns as comprehensibility; it remains unclear which specific linguistic dimensions primarily drive listeners' perceptions of perceived fluency.


\subsection{Task Effects}

\subsubsection{Task Complexity Manipulation} %TODO: change the section name



To investigate task effects on listener judgments further, researchers have often utilized Robinson's (\citeyear{Robinson2005}) framework of task classification to explore the relationship between listeners’ judgments and the linguistic dimensions of tasks of varying complexity. This framework suggests that the resource-directing dimensions of tasks, such as ±here and now, ±few elements, and ±intentional reasoning, lead to varying cognitive load. However, such studies share a common methodological limitation: the reliance on picture narrative tasks \parencite{SuzukiKormos2020}, failing to reflect real-world communication's varied cognitive load. Recent studies have begun investigating whether task complexity could affect the relationship between linguistic dimensions and listeners' judgments \parencite[e.g.,][]{Bergeron2017,crowther2018,fullana2024,SaitoLiu2022}. Nevertheless, as \textcite{sasayama2016} notes, designed differences between tasks of varying complexity may not always translate to actual cognitive differences. While some recent studies have attempted to address this issue \parencite[e.g.,][]{crowther2018,fullana2024}, other previous studies rely solely on Robinson's (\citeyear{Robinson2005}) framework for classifying task complexity \parencite[e.g.,][]{Bergeron2017,SaitoLiu2022}.

Two studies have investigated the impact of task complexity on the relationship between listeners' judgments and a broad range of linguistic dimensions \parencite{Bergeron2017,crowther2018}. \textcite{Bergeron2017} compared a less complex picture narrative task (+few elements, +prior knowledge, +here and now, +planning) with a more complex interview task ($-$few elements, $-$planning, $-$here and now). Following Robinson's (\citeyear{Robinson2005}) framework, the authors classified the interview task as placing greater cognitive load on L2 learners. In the less complex task, accentedness was related to pronunciation and fluency, while comprehensibility was also related to lexis, grammar, and discourse. In the more complex task, both accentedness and comprehensibility were linked to a broader, similar set of linguistic dimensions. This ``task-specific'' effect underscores that the relative importance of different linguistic dimensions to listeners' judgments is not fixed; rather, it varies with the cognitive load imposed by the task.


%TODO: move the resource dimensions things to chapter 2
\textcite{crowther2018} utilized three tasks of varying complexity: a Picture Task (+few elements, +spatial knowledge, +here/now, $-$causal reasoning, $-$intentional reasoning, $-$perspective-taking), an IELTS Task (+few elements, +spatial knowledge, $-$here/now, $-$causal reasoning, $-$intentional reasoning, $-$perspective-taking), and a TOEFL Task ($-$few elements, $-$spatial knowledge, $-$here/now, +causal reasoning, +intentional reasoning, +perspective-taking). Following Robinson's (\citeyear{Robinson2005}) framework, the authors rated TOEFL task as the most complex, then IELTS, and finally the picture task. L2 speakers' perceived difficulty supported this classification, with the TOEFL task rated as more difficult than both the picture and IELTS tasks, thus validating the task complexity manipulation. No significant difference in perceived difficulty was found between the latter two. Their results showed that comprehensibility was consistently related to pronunciation and lexicogrammar. In contrast, only lexical appropriateness and grammatical accuracy correlated with accentedness in the picture task, but all five lexicogrammar measures were associated with accentedness in the more complex IELTS and TOEFL tasks. Thus, the linguistic correlates of comprehensibility remained stable, whereas the linguistic correlates of accentedness changed with task complexity. This finding diverges from \textcite{Bergeron2017}, who reported that linguistic correlates of comprehensibility were also task-specific.


\section{Research Questions}

This study examines how task complexity changes what listeners attend to when they rate L2 Japanese speech. Drawing from earlier research showing that task demands can influence the relevance of linguistic dimensions in listeners' judgments \parencite[e.g.,][]{Bergeron2017,crowther2018}, we formulated three research questions and three corresponding hypotheses.


\begin{description}
    \item[RQ1:] To what extent are comprehensibility and perceived fluency ratings related and distinct when native listeners evaluate L2 speech produced in tasks of different complexity?
\end{description}

Based on the construct overlap between comprehensibility and perceived fluency, we hypothesize:

\begin{description}
    \item[H1:] Comprehensibility and perceived fluency will be strongly correlated in both low and high complexity tasks, and their mean levels will not differ substantially.

\end{description}

\begin{description}
    \item[RQ2:] How do linguistic dimensions correlate with comprehensibility across low and high complexity tasks?
\end{description}

Based on findings that listeners rely on a broader set of linguistic dimensions when judging comprehensibility from complex tasks compared to less complex ones \parencite{Bergeron2017}, we hypothesize a similar task-specific effect:

\begin{description}
    \item[H2:] In the low complexity task, listeners will focus on a relatively narrow set of salient linguistic dimensions (for example, clause final pausing and accuracy) because speakers can produce more balanced, fluent speech. In the high complexity task, where speaker production is more compromised, listeners will draw on a broader set of linguistic dimensions in order to reconstruct the intended message.
\end{description}

\begin{description}
    \item[RQ3:] How do linguistic dimensions correlate with perceived fluency across low and high complexity tasks?
\end{description}

Conversely, because listeners inherently make fluency judgments based on selected utterance features \parencite{Segalowitz2010}, we hypothesize:

\begin{description}
    \item[H3:] Perceived fluency will remain linked to a comparatively narrow set of linguistic dimensions even under the more complex task, and will broaden less than comprehensibility.
\end{description}


\section{Statistical Analysis}
Table \ref{tab:descriptive_stats_study2_low} and Table \ref{tab:descriptive_stats_study2_high} present the descriptive statistics for the judgment scores and linguistic measures. To investigate the relationship between comprehensibility and perceived fluency, we conducted separate analyses for the low- and high-complexity tasks. Normality checks using the Shapiro-Wilk test indicated non-normality only for comprehensibility scores in the high-complexity task. Consequently, parametric tests (t-test, Pearson correlation) were used for analyses within the low-complexity task. For the high-complexity task, the corresponding non-parametric tests (Wilcoxon signed-rank test, Spearman correlation) were employed. Effect sizes for these analyses were calculated and interpreted following \citeauthor{plonsky2014}'s (\citeyear{plonsky2014}) guidelines.

\begin{table}[ht]
\centering
\caption{Descriptive statistics of comprehensibility, perceived fluency, and linguistic dimensions (Low-complexity task)}
\footnotesize
\label{tab:descriptive_stats_study2_low}
% Set S column formats to align all numbers
\begin{tabular}{l
                S[table-format=3.2]
                S[table-format=2.2]
                S[table-format=3.2]
                S[table-format=3.2]}
\toprule
& & & \multicolumn{2}{c}{$95\%$ CI} \\
\cline{4-5}
% Wrap S-column headers in {}
\textsc{Measures} & {$M$} & {$SD$} & {\textit{Lower}} & {\textit{Upper}} \\
\hline
% Use \multicolumn for category headers
\multicolumn{5}{l}{\textit{Listeners' judgments}} \\
Comprehensibility & 5.73 & 1.60 & 5.54 & 5.92 \\
Perceived fluency & 5.84 & 1.60 & 5.65 & 6.03 \\
\multicolumn{5}{l}{\textit{Syntactic complexity}} \\
Clause per AS-unit & 1.44 & 0.39 & 1.31 & 1.58 \\
Case particles per AS-unit & 1.77 & 0.70 & 1.53 & 2.00 \\
Conjunctive particles per AS-unit & 0.94 & 0.64 & 0.72 & 1.16 \\
Adverbial particles per AS-unit & 1.17 & 0.50 & 1.00 & 1.34 \\
Auxiliary verbs per AS-unit & 1.29 & 0.79 & 1.02 & 1.56 \\
\multicolumn{5}{l}{\textit{Lexical complexity}} \\
Lexical density & 46.33 & 3.65 & 45.08 & 47.58 \\
Different content words per AS-unit & 4.65 & 1.50 & 4.13 & 5.16 \\
Number of content words per AS-unit & 6.35 & 1.83 & 5.72 & 6.98 \\
\multicolumn{5}{l}{\textit{Accuracy}} \\
Error rate & 0.91 & 0.59 & 0.71 & 1.11 \\
\multicolumn{5}{l}{\textit{Speed fluency}} \\
Speech rate & 203.97 & 47.62 & 187.62 & 220.33 \\
Articulation rate & 320.93 & 50.76 & 303.49 & 338.36 \\
Mean length of run & 7.41 & 1.62 & 6.85 & 7.96 \\
\multicolumn{5}{l}{\textit{Breakdown fluency}} \\
Mid-clause pause duration & 0.69 & 0.22 & 0.61 & 0.76 \\
Mid-clause pause ratio & 0.20 & 0.08 & 0.17 & 0.23 \\
Final-clause pause duration & 0.89 & 0.30 & 0.78 & 0.99 \\
Final-clause pause ratio & 0.13 & 0.04 & 0.12 & 0.15 \\
Filled pause ratio & 0.13 & 0.06 & 0.11 & 0.15 \\
\multicolumn{5}{l}{\textit{Repair fluency}} \\
Dysfluency rate & 1.78 & 1.45 & 1.28 & 2.27 \\
\bottomrule
\end{tabular}
\end{table}


\begin{table}[ht]
\centering
\caption{Descriptive statistics of comprehensibility, perceived fluency, and linguistic dimensions (High-complexity task)}
\footnotesize
\label{tab:descriptive_stats_study2_high}
% Set S column formats to align all numbers (same as other table for consistency)
\begin{tabular}{l
                S[table-format=3.2]
                S[table-format=2.2]
                S[table-format=3.2]
                S[table-format=3.2]}
\toprule
& & & \multicolumn{2}{c}{$95\%$ CI} \\
\cline{4-5}
% Wrap S-column headers in {}
\textsc{Measures} & {$M$} & {$SD$} & {\textit{Lower}} & {\textit{Upper}} \\
\hline
% Use \multicolumn for category headers
\multicolumn{5}{l}{\textit{Listeners' judgments}} \\
Comprehensibility & 5.51 & 1.65 & 5.31 & 5.70 \\
Perceived fluency & 5.59 & 1.67 & 5.40 & 5.79 \\
\multicolumn{5}{l}{\textit{Syntactic complexity}} \\
Clause per AS-unit & 1.33 & 0.22 & 1.26 & 1.41 \\
Case particles per AS-unit & 1.51 & 0.76 & 1.24 & 1.77 \\
Conjunctive particles per AS-unit & 0.64 & 0.46 & 0.48 & 0.80 \\
Adverbial particles per AS-unit & 0.90 & 0.40 & 0.77 & 1.04 \\
Auxiliary verbs per AS-unit & 1.45 & 0.65 & 1.22 & 1.67 \\
\multicolumn{5}{l}{\textit{Lexical complexity}} \\
Lexical density & 48.54 & 4.39 & 47.03 & 50.04 \\
Different content words per AS-unit & 4.13 & 1.01 & 3.79 & 4.48 \\
Number of content words per AS-unit & 5.62 & 1.40 & 5.14 & 6.10 \\
\multicolumn{5}{l}{\textit{Accuracy}} \\
Error rate & 0.68 & 0.53 & 0.50 & 0.86 \\
\multicolumn{5}{l}{\textit{Speed fluency}} \\
Speech rate & 185.37 & 49.68 & 168.31 & 202.44 \\
Articulation rate & 300.64 & 47.52 & 284.32 & 316.97 \\
Mean length of run & 7.35 & 1.64 & 6.78 & 7.91 \\
\multicolumn{5}{l}{\textit{Breakdown fluency}} \\
Mid-clause pause duration & 0.73 & 0.23 & 0.65 & 0.81 \\
Mid-clause pause ratio & 0.19 & 0.08 & 0.16 & 0.22 \\
Final-clause pause duration & 1.02 & 0.40 & 0.88 & 1.16 \\
Final-clause pause ratio & 0.16 & 0.06 & 0.14 & 0.18 \\
Filled pause ratio & 0.14 & 0.06 & 0.13 & 0.16 \\
\multicolumn{5}{l}{\textit{Repair fluency}} \\
Dysfluency rate & 1.87 & 1.49 & 1.36 & 2.38 \\
\bottomrule
\end{tabular}
\end{table}




We used linear mixed-effects models to investigate how linguistic dimensions shape comprehensibility and perceived fluency. Separate models were run for low- and high-complexity tasks to identify the potentially distinct sets of linguistic dimensions driving comprehensibility and perceived fluency within each task condition. Due to multicollinearity, number of content words per AS-unit, articulation rate, and mean length of run were excluded as predictors. For each outcome (comprehensibility or perceived fluency) under low- and high-complexity, we fit a full model using the lme4 package \parencite{bates2015} with all remaining linguistic dimensions as fixed effects, random intercepts for speaker and rater to control for baseline differences across speakers and overall rating tendencies among raters, and covariates for rater familiarity with Chinese accented speech and teaching experience. Model selection proceeded in two stages: (1) the dredge function from the MuMIn package \parencite{barton2024} generated all predictor combinations; (2) backward stepwise selection using step minimized Akaike Information Criterion (AIC). We chose the most parsimonious model by the lowest Schwarz's Bayesian Information Criterion (BIC). To test each predictor's unique contribution, a drop-one analysis was performed by systematically removing each fixed effect one at a time from the final model and comparing the resulting reduced model to the full model using likelihood ratio tests, changes in AIC, and marginal $R^2_m$. Fixed effects whose removal significantly increased AIC and lowered $R^2_m$ were considered essential. We then performed power analyses using the simr package \parencite{green2016} by running 1000 simulations for each remaining fixed effect. Power was defined as the percentage of simulations achieving significance ($p < .05$), reflecting the probability of detecting the effect given our model and sample size. Effect sizes were interpreted following \citeauthor{plonsky2018}'s (\citeyear{plonsky2018}) guidelines.


\section{Results}



\subsection{Interrater Reliability}
Interrater reliability was assessed separately for each task using Cronbach's alpha and Intraclass Correlation Coefficients (ICCs) (Table \ref{tab:interrater_reliability_study2}). High Cronbach's alpha values ($.87$--$.91$) were observed, reflecting overall consistency in the ratings. The ICCs offer specific indices for single-rater and average-rater consistency and agreement, allowing for a more detailed analysis. Focusing on the most relevant ICC forms for our design (ICC3/ICC3k, reflecting consistency with the same set of raters), the moderate ICC3 values ($.47$--$.56$) suggest that the consistency of any single rater, chosen at random, was moderate. However, the excellent ICC3k values ($.87$--$.91$) demonstrate that the averaged ratings from all eight raters provided a highly consistent and reliable measure of speaker comprehensibility and perceived fluency across both tasks.


\begin{table}[ht]
\centering
\begin{threeparttable}
\caption{Interrater reliability statistics for comprehensibility and perceived fluency}

\footnotesize
\label{tab:interrater_reliability_study2}
\begin{tabular}{lccccccc}
\toprule
& {$\alpha$} & {ICC$_1$} & {ICC$_2$} & {ICC$_3$} & {ICC$_{1k}$} & {ICC$_{2k}$} & {ICC$_{3k}$} \\
\hline
\multicolumn{8}{l}{\textit{Low-complexity task}} \\
Comprehensibility & $.88$ & $.38$ & $.39$ & $.47$ & $.83$ & $.84$ & $.88$ \\
Perceived Fluency & $.90$ & $.39$ & $.40$ & $.53$ & $.83$ & $.84$ & $.90$ \\
\multicolumn{8}{l}{\textit{High-complexity task}} \\
Comprehensibility & $.87$ & $.33$ & $.35$ & $.46$ & $.80$ & $.81$ & $.87$ \\
Perceived Fluency & $.91$ & $.41$ & $.43$ & $.56$ & $.85$ & $.86$ & $.91$ \\
\bottomrule
\end{tabular}
\begin{tablenotes}
\footnotesize
\item \textit{Note.} ICC$_1$ = One-way random effects (assumes different random raters per target; measures absolute agreement); ICC$_2$ = Two-way random effects (assumes same random raters per target; measures absolute agreement); ICC$_3$ = Two-way mixed effects (assumes same specific raters per target; measures consistency). Suffix \textit{k} refers to the reliability of the average rating across \textit{k} raters, while no suffix refers to single rater reliability.
\end{tablenotes}
\end{threeparttable}
\end{table}



\subsection{RQ1: Relationship Between Comprehensibility and Perceived Fluency}
To examine whether comprehensibility and perceived fluency are distinct constructs, we conducted correlational and mean-comparison analyses. In the low-complexity task, a Pearson correlation showed a strong positive link between comprehensibility and perceived fluency ($r(33) = .94$, $p < .001$, $95\%$ CI $[0.88, 0.97]$, $d = 5.51$). A paired \textit{t}-test found no significant difference between comprehensibility ($M = 5.73$, $SD = 1.60$) and perceived fluency ($M = 5.84$, $SD = 1.60$; $t(34) = -1.79$, $p = .083$, $95\%$ CI $[-0.24, 0.02]$, $d = -0.30$). In the high-complexity task, a Spearman correlation indicated a strong relationship ($\rho(33) = .93$, $p < .001$, $95\%$ CI $[0.86, 0.96]$, $d = 4.91$), and a Wilcoxon test showed no significant difference between comprehensibility ($M = 5.51$, $SD = 1.65$) and perceived fluency ($M = 5.59$, $SD = 1.67$; $V = 130.5$, $p = .099$, $d = -0.23$).


\subsection{RQ2: Linguistic Dimensions of Comprehensibility} 
\label{study2_comprehensibility}
To investigate how linguistic dimensions shape comprehensibility across different levels of task complexity, we ran separate mixed-effects models and verified key assumptions. Shapiro-Wilk tests indicated no significant deviations from normality ($p > .05$), residual-versus-fitted plots indicated no heteroscedasticity, and all Variance Inflation Factors (VIF) stayed below 4. Random intercepts for speaker and rater had non-zero variances, suggesting no issues. These diagnostic checks suggested that the model assumptions were adequately met, and the results can be considered valid. Table \ref{tab:comprehensibility_models_study2} presents the best-fitting comprehensibility models for both tasks.




\begin{table}[ht]
\centering
\begin{threeparttable}
\setlength{\tabcolsep}{4pt}
\caption{Fixed effects, drop-one analysis, and power estimates for the comprehensibility models}

\footnotesize
\label{tab:comprehensibility_models_study2}
\begin{tabular}{l
                S[table-format=-0.2]
                S[table-format=0.2]
                S[table-format=-1.2]
                c
                S[table-format=0.3]
                S[table-format=+1.2]
                S[table-format=2.2]
                c
                S[table-format=2.2]}
\toprule
Fixed effects & {$\beta$} & {\textit{SE}} & {\textit{t}} & {\textit{p}} & {$\Delta R^2$} & {$\Delta$AIC} & {LRT} & {LRT} & {Power} \\
& & & & & & & {$\chi^2(1)$} & {\textit{p}} & {(\%)} \\
\midrule
\multicolumn{10}{l}{\textit{Low-complexity task}} \\
FinalPR & -0.61 & 0.16 & -3.85 & $<.001$ & 0.124 & +8.67 & 13.05 & $<.001$ & 96.40 \\
ER & -0.51 & 0.16 & -3.24 & $\phantom{<}.003$ & 0.088 & +5.50 & 9.75 & $\phantom{<}.002$ & 87.70 \\
\multicolumn{10}{l}{\textit{High-complexity task}} \\
C/AS & 0.79 & 0.21 & 3.80 & $<.001$ & 0.100 & +8.75 & 14.25 & $<.001$ & 94.90 \\
FinalPR & -0.70 & 0.20 & -3.50 & $\phantom{<}.002$ & 0.084 & +7.11 & 12.41 & $<.001$ & 92.50 \\
DCW/AS & -0.69 & 0.25 & -2.78 & $\phantom{<}.010$ & 0.053 & +4.10 & 8.34 & $\phantom{<}.004$ & 76.30 \\
AuxV/AS & -0.68 & 0.22 & -3.11 & $\phantom{<}.004$ & 0.067 & +5.41 & 10.17 & $\phantom{<}.001$ & 84.20 \\
CaseP/AS & 0.45 & 0.19 & 2.33 & $\phantom{<}.027$ & 0.037 & +1.69 & 6.08 & $\phantom{<}.014$ & 62.90 \\
DysfRate & -0.39 & 0.15 & -2.60 & $\phantom{<}.014$ & 0.046 & +2.29 & 7.41 & $\phantom{<}.007$ & 71.70 \\
\bottomrule
\end{tabular}
\begin{tablenotes}
\footnotesize
\item \textit{Note.} FinalPR = final-clause pause ratio. ER = error rate. C/AS = clauses per AS-unit. DCW/AS = different content words per AS-unit. AuxV/AS = auxiliary verbs per AS-unit. CaseP/AS = case particles per AS-unit. DysfRate = dysfluency rate. All continuous predictors were standardized. $\Delta R^2$ represents the change in $R^2_m$ when each predictor is added to a null model. $\Delta$AIC shows the increase in the Akaike Information Criterion if the predictor is removed from the full model. LRT $\chi^2(1)$ and \textit{p} values are from likelihood ratio tests comparing the full model versus a model without the given predictor. Power (\%) refers to observed power.
\end{tablenotes}
\end{threeparttable}
\end{table}



\subsubsection{Comprehensibility Model in Low-complexity Task}
Table \ref{tab:comprehensibility_models_study2} shows the low-complexity task's comprehensibility model with two significant fixed effects. Increased final-clause pause ratio ($\Delta R^2 = .124$) and error rate ($\Delta R^2 = .088$) reduced comprehensibility. Random effects analysis revealed significant variance in comprehensibility scores attributable to speaker (Variance $= 0.64$, $SD = 0.80$) and rater variation (Variance $= 0.43$, $SD = 0.65$), with residual variance at 1.17 ($SD = 1.08$). The model explained 16.4\% of the variance in comprehensibility through fixed effects alone ($R^2_m = .164$) and 56.2\% when random effects were included (conditional $R^2$, $R^2_c = .562$). According to \citeauthor{plonsky2014}'s (\citeyear{plonsky2014}) guidelines, these indicate small and large effects, respectively. Drop-one analyses showed that removing either predictor significantly worsened model fit, confirming their independent contributions (see Table \ref{tab:comprehensibility_models_study2} for details). Power analyses indicated high observed power for both final-clause pause ratio (96.40\%) and error rate (87.70\%), suggesting robust detection of their effects.

\subsubsection{Comprehensibility Model in High-complexity Task}
As also shown in Table \ref{tab:comprehensibility_models_study2}, the high-complexity task's comprehensibility model revealed six significant fixed effects. Increased clauses per AS-unit ($\Delta R^2 = .100$) and case particles per AS-unit ($\Delta R^2 = .037$) enhanced comprehensibility, while higher final-clause pause ratio ($\Delta R^2 = .084$), auxiliary verbs per AS-unit ($\Delta R^2 = .067$), lexical diversity (different content words per AS-unit, $\Delta R^2 = .053$), and dysfluency rate ($\Delta R^2 = .046$) reduced it. Random effects analysis revealed considerable variance in comprehensibility scores attributable to speaker (Variance $= 0.55$, $SD = 0.74$) and rater variation (Variance $= 0.65$, $SD = 0.81$), with residual variance at 1.19 ($SD = 1.09$). The model accounted for 18.9\% of variance through fixed effects ($R^2_m = .189$) and 59.7\% with random effects ($R^2_c = .597$), representing small and large effects, respectively. Drop-one analyses confirmed each effect's contribution, with power analyses showing high power for clauses per AS-unit (94.90\%), final-clause pause ratio (92.50\%), and adequate power for others (62.90\%--84.20\%).

\subsection{Linguistic Dimensions of Perceived Fluency}
Following the steps in Section \ref{study2_comprehensibility}, we fitted mixed-effects models to assess fixed effects on perceived fluency in each task. Table \ref{tab:perceived_fluency_models_study2} presents the best-fitting perceived fluency models for both tasks.

\begin{table}[ht]
\centering
\begin{threeparttable}
\setlength{\tabcolsep}{4pt}
\caption{Fixed effects, drop-one analysis, and power estimates for the perceived fluency models}

\footnotesize
\label{tab:perceived_fluency_models_study2}
\begin{tabular}{l
                S[table-format=-0.2]  % beta
                S[table-format=0.2]   % SE
                S[table-format=-2.2]  % t (for -12.16)
                c % p
                S[table-format=0.3]   % Delta R^2
                S[table-format=+1.2]  % Delta AIC
                S[table-format=2.2]   % LRT Chi^2
                c % p
                S[table-format=2.2]}  % Power
\toprule
Fixed effects & {$\beta$} & {\textit{SE}} & {\textit{t}} & {\textit{p}} & {$\Delta R^2$} & {$\Delta$AIC} & {LRT} & {LRT} & \multicolumn{1}{c}{Power} \\
& & & & & & & {$\chi^2(1)$} & {\textit{p}} & \multicolumn{1}{c}{(\%)} \\
\midrule
\multicolumn{10}{l}{\textit{Low-complexity task}} \\
FinalPR & -0.59 & 0.16 & -12.16 & $<.001$ & 0.120 & +7.86 & 12.16 & $<.001$ & 95.50 \\
ER & -0.52 & 0.16 & -3.24 & $\phantom{<}.003$ & 0.083 & +5.60 & 9.81 & $\phantom{<}.002$ & 87.80 \\
\multicolumn{10}{l}{\textit{High-complexity task}} \\
C/AS & 0.55 & 0.21 & 6.75 & $\phantom{<}.014$ & 0.069 & +3.01 & 6.75 & $\phantom{<}.009$ & 71.30 \\
AuxV/AS & -0.49 & 0.26 & 3.85 & $\phantom{<}.064$ & 0.054 & +0.70 & 3.85 & $\phantom{<}.050$ & 46.80 \\
FinalPR & -0.45 & 0.23 & 3.89 & $\phantom{<}.063$ & 0.065 & +0.52 & 3.89 & $\phantom{<}.049$ & 45.30 \\
\bottomrule
\end{tabular}
\begin{tablenotes}
\footnotesize
\item \textit{Note.} FinalPR = final-clause pause ratio. ER = error rate. C/AS = clauses per AS-unit. AuxV/AS = auxiliary verbs per AS-unit. All continuous predictors were standardized.
\end{tablenotes}
\end{threeparttable}
\end{table}

\subsubsection{RQ3: Perceived Fluency Model in Low-complexity Task}
Table \ref{tab:perceived_fluency_models_study2} shows the low-complexity task's perceived fluency model with two significant negative fixed effects. Increased final-clause pause ratio ($\Delta R^2 = .120$) and error rate ($\Delta R^2 = .083$) reduced perceived fluency. Random effects analysis revealed considerable variance in fluency scores attributable to speaker (Variance $= 0.69$, $SD = 0.83$) and rater variation (Variance $= 0.63$, $SD = 0.79$), with residual variance at 0.96 ($SD = 0.98$). The model explained 15.9\% of variance through fixed effects ($R^2_m = .159$) and 64.5\% with random effects ($R^2_c = .645$), representing small and large effects, respectively. Drop-one analyses confirmed both effects' contributions, with high power for final-clause pause ratio (95.50\%) and error rate (87.80\%), indicating robust detection.

\subsubsection{Perceived Fluency Model in High-complexity Task}
As also shown in Table \ref{tab:perceived_fluency_models_study2}, the high-complexity task's perceived fluency model revealed one significant fixed effect and two marginally significant effects. Increased clauses per AS-unit ($\Delta R^2 = .069$) enhanced perceived fluency, while final-clause pause ratio and auxiliary verbs per AS-unit suggested slight reductions. Random effects analysis revealed considerable variance in perceived fluency scores attributable to speaker (Variance $= 0.99$, $SD = 1.00$) and rater variation (Variance $= 0.67$, $SD = 0.82$), with residual variance at 0.99 ($SD = 1.00$). The model accounted for 11.2\% of variance through fixed effects ($R^2_m = .112$) and 66.9\% with random effects ($R^2_c = .669$), representing small and large effects, respectively. While removing the final-clause pause ratio and auxiliary verbs per AS-unit led to slight increases in AIC and trend-level LRT values, suggesting potential contributions, power analyses revealed low observed power for these two fixed effects (45.30\% and 46.80\%, respectively). Power for clauses per AS-unit was adequate (71.30\%).

\section{Discussion}

\subsection{RQ1: Relationship Between Comprehensibility and Perceived Fluency}
Addressing the first research question, strong positive correlations were revealed between comprehensibility and perceived fluency across both low- ($r = .94$, $p < .001$) and high-complexity tasks ($\rho = .93$, $p < .001$). This strong correlation replicates findings from prior research \parencite{SuzukiKormos2020}. However, regarding the distinctness of these judgments, our analyses revealed no significant difference between the scores in either task ($p > .05$), indicating listeners made little distinction. This finding contrasts with \textcite{SuzukiKormos2020}, who report a significant difference between the scores. This divergence might result from differing rater experience (experienced in this study vs. inexperienced in their study) or fluency assessed (narrow sense in this study vs. broad sense in theirs). Consequently, for this study's experienced raters, comprehensibility and perceived fluency were strongly related but not practically distinct, regardless of task complexity.

\subsection{RQ2: Linguistic Correlates of Comprehensibility Across Task Complexity}
\label{RQ2discussion_study}
When investigating how linguistic dimensions relate to L2 Japanese comprehensibility, our findings showed that in the low-complexity task, final-clause pause ratio and error rate were significant negative fixed effects, suggesting that in less complex tasks, these salient features are primary cues listeners use to judge the ease of understanding. This aligns with previous research highlighting breakdown fluency \parencite{kang2010} and accuracy \parencite{DerwingMunro2015} as key indicators of comprehensibility.

However, the relationship between linguistic dimensions and comprehensibility became more multifaceted in the high-complexity task. Although final-clause pause ratio still negatively affected comprehensibility, clauses per AS-unit and case particles per AS-unit positively predicted it, suggesting that under higher cognitive load, listeners rely on elaborate syntactic structures and explicit grammatical markers to accurately decode the speaker's intended message. Conversely, dysfluency rate, different content words per AS-unit, and auxiliary verbs per AS-unit emerged as negative fixed effects. These results differ from the findings from \citeauthor{saito2017}'s (\citeyear{saito2017}) study on L2 Japanese comprehensibility, whose linear mixed-effects model identified lexical variation as a significant predictor of comprehensibility. Their correlation analysis also confirmed that greater lexical variation was positively associated with comprehensibility. One likely explanation for this discrepancy lies in the difference in task complexity: whereas \textcite{saito2017} investigated picture narrative tasks, our high-complexity task places greater cognitive load on speakers and listeners, potentially overshadowing the benefits of lexical variation.

The shift of the linguistic correlates of comprehensibility parallels \citeauthor{Skehan2009}'s (\citeyear{Skehan2009}) limited attentional capacity model, where speakers trade off complexity, fluency, and accuracy due to limited resources. Our findings reveal a parallel effect in perception: increased task complexity shifts listeners' prioritization of linguistic dimensions when judging L2 speech. On the one hand, greater syntactic sophistication can enhance comprehensibility in high-complexity tasks; on the other, excessive verb constructions or high lexical variation may overburden listeners' processing capacities under more demanding conditions.

Another noteworthy finding that contrasts with prior research concerns the role of pause location in comprehensibility. While \textcite{SuzukiKormos2020} found mid-clause pausing to be a primary cue for comprehensibility in English, only final-clause pausing was a significant predictor in the current study. A likely explanation for this lies in Japanese's Subject-Object-Verb (SOV) structure. Because key morphological and semantic information (e.g., the main verb, tense/aspect markers, sentence-final particles, conjunctive particles) is often reserved for the end of a clause, pauses at this juncture force listeners to hold incomplete information in working memory. This delay in message resolution may hinder the accurate interpretation of the entire utterance.

\subsection{RQ3: Linguistic Correlates of Perceived Fluency Across Task Complexity}
Analyses of perceived fluency revealed a similar pattern to comprehensibility in the low-complexity task, with final-clause pause ratio and error rate emerging as significant negative fixed effects. Listeners in a simpler communicative environment appear to judge perceived fluency by the degree of temporal smoothness and absence of grammatical or lexical errors. These findings align with \textcite{cucchiarini2002} and \textcite{SuzukiKormos2020}, who found that breakdown fluency and accuracy are significant predictors shaping perceived fluency in broad sense.

In the high-complexity task, the fixed effects of perceived fluency shifted. Clauses per AS-unit showed a significant positive relationship, indicating that elaborate syntactic structures can be interpreted as a hallmark of fluent speech when speakers tackle complex content. Although final-clause pause ratio and auxiliary verbs per AS-unit exhibited only trend-level negative effects, they suggest that prolonged pausing and potentially convoluted verb constructions may still detract from perceived fluency in more demanding communicative scenarios. This holds true even if listeners become somewhat more tolerant of pauses when speakers handle complex ideas. Similarly, the shift of linguistic correlates of perceived fluency also parallels \citeauthor{Skehan2009}'s (\citeyear{Skehan2009}) model, reflecting the similar pattern of listener's prioritization to cognitive load seen for comprehensibility.

These results align with \citeauthor{Segalowitz2010}'s (\citeyear{Segalowitz2010}) multifaceted view of fluency, which holds that although temporal features remain crucial, they are also shaped by broader cognitive and linguistic sophistication that influences listeners' overall perceptions. However, like the pattern observed for comprehensibility, final-clause pauses significantly influenced perceived fluency. This pattern contrasts with many studies of English, where mid-clause pauses tend to be most detrimental \parencite{suzuki2021}. As explained in Section \ref{RQ2discussion_study}, the SOV structure of Japanese concentrates crucial semantic and morphological information at the end of the clause. Therefore, disruptions at this boundary may become highly salient and detrimental to fluency for listeners. Ultimately, these findings confirm that perceived fluency is sensitive to surface-level markers of breakdown (especially at syntactic boundaries in less complex tasks) and to deeper syntactic complexity (notably in more complex tasks), reflecting a shift in fluency dimensions as communicative demands escalate.

\section{Conclusion}
This study investigated how task complexity differentially modulates the linguistic dimensions influencing comprehensibility and perceived fluency in L2 Japanese. Although listeners made little practical distinction between the judgments, their underlying linguistic correlates shifted significantly with task complexity, confirming our initial hypotheses that these correlates shift in different ways as task demands increase. These findings resonate with \citeauthor{skehan1997}'s (\citeyear{skehan1997}) limited attentional capacity model. Specifically, as task complexity increases and speakers produce more elaborate speech, listeners trade off prioritizing linguistic dimensions, as normally advantageous features may overload processing under higher demand. Crucially, the current study highlighted the unique role of function words in shaping listeners' judgments, reflecting their reliance on morphological cues to decode meaning in Japanese. Meanwhile, pauses at clause boundaries reduced fluency in an SOV language where extra breaks seem unnecessary or confusing.


