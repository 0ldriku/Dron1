\section{Introduction}
Effective communication in a second language (L2) is a collaborative process depending not only on the speaker's ability to produce comprehensible and fluent speech but also on the listener's capacity to properly process and interpret the message \parencite{baeseberk2020}. Comprehensibility---the listener's perception of how easily they understand L2 speech \parencite{MunroDerwing1999}---is crucial for communicative success in real-world interactions \parencite{DerwingMunro2009} and is assessed in high-stakes language tests such as IELTS and TOEFL \parencite{IsaacsTrofimovich2012}. Alongside comprehensibility, fluency is also considered an essential component for successful L2 communication \parencite{suzuki2021}. \textcite{lennon1990,lennon2000} distinguished two senses of fluency: fluency in the broad sense (overall oral proficiency) and fluency in the narrow sense (smoothness of speech). Besides this distinction, \textcite{Segalowitz2010} proposed a framework to better integrate various facets of fluency such as listener-based judgements of fluency and temporal features of speech. He distinguished among utterance fluency (observable temporal features), cognitive fluency (underlying psycholinguistic processes), and perceived fluency (listeners' inferences about cognitive fluency based on utterance fluency features).

To help L2 learners improve the comprehensibility and fluency of their L2 speech, a growing body of research has investigated which linguistic dimensions underlie listeners' perceptions. Various studies have proposed that these constructs are tied to a broad set of linguistic dimensions, including phonology, temporal fluency, lexicon, grammar, and discourse \parencite[e.g.,][]{crowther2015}. However, such studies share a common methodological limitation: the reliance on picture narrative tasks \parencite{SuzukiKormos2020}, failing to reflect real-world communication's varied cognitive demands. Recent studies have begun investigating whether task complexity could affect the relationship between linguistic dimensions and listeners' judgments \parencite[e.g.,][]{Bergeron2017,crowther2018,fullana2024,SaitoLiu2022}. Nevertheless, as \textcite{sasayama2016} notes, designed differences between tasks of varying complexity may not always translate to actual cognitive differences. While some recent studies have attempted to address this issue \parencite[e.g.,][]{crowther2018,fullana2024}, other previous studies rely solely on \textcite{Robinson2005}'s framework for classifying task complexity \parencite[e.g.,][]{Bergeron2017,SaitoLiu2022}.

Drawing from earlier research showing that task demands can influence the relevance of linguistic dimensions in listeners' judgments \parencite[e.g.,][]{Bergeron2017,crowther2018}, this study explores how task complexity impacts the linguistic dimensions of comprehensibility and perceived fluency in L2 Japanese. Based on findings that listeners rely on a broader set of linguistic dimensions when judging comprehensibility from complex tasks compared to less complex ones \parencite{Bergeron2017}, we hypothesize a similar task-specific effect. Specifically, we predict that in less complex tasks, listeners focus on a relatively narrow set of linguistic dimensions because speakers can produce more balanced, fluent speech. In contrast, in more complex tasks, speakers' compromised speech production leads listeners to draw on a broader range of linguistic dimensions to reconstruct the intended message. Conversely, because listeners inherently make fluency judgments based on selected utterance features \parencite{Segalowitz2010}, we hypothesize that perceived fluency will remain linked to a comparatively narrow set of linguistic dimensions, even under more complex tasks.


\section{Literature review}

\subsection{Relationship between comprehensibility and fluency}
Researchers have long investigated the relationship between L2 comprehensibility and fluency. While numerous studies confirm that utterance fluency strongly correlates with comprehensibility ratings \parencite[e.g.,][]{IsaacsTrofimovich2012,SaitoShintani2016}, the specific link between comprehensibility and perceived fluency has remained comparatively underexplored. \textcite{SuzukiKormos2020} offer one of the most direct investigations into the relationship between comprehensibility and perceived fluency. They found inexperienced raters are more lenient when judging comprehensibility than perceived fluency (in its broad sense), attributing this to different evaluation standards: comprehensibility tied to ease of understanding \parencite{saito2016a}, while fluency judgments often rely on comparisons to native-speaker proficiency \parencite{tavakoli2018}. The present study shifts the focus, specifically investigating the relationship between comprehensibility and perceived fluency defined in a narrow sense, the smoothness and temporal regularity of speech.

\subsection{Linguistic dimensions of comprehensibility and perceived fluency}

\subsubsection{Comprehensibility}
Extensive research in second language acquisition has explored the linguistic dimensions that underlie comprehensibility. At the segmental level, errors in high-functional-load consonants significantly impair comprehensibility \parencite{MunroDerwing2006}. At the suprasegmental level, prosodic features, such as sentence stress \parencite{hahn2004}, word stress \parencite{IsaacsTrofimovich2012}, and pitch range and pausing \parencite{kang2010} are linked to comprehensibility. As for lexical and grammatical aspects, studies indicate that low grammatical accuracy \parencite{MunroDerwing1995} and poor word choice \parencite{fayer1987} can negatively affect comprehensibility. More recently, \textcite{saito2016a} have highlighted that comprehensibility in L2 English is linked to phonology, grammar, vocabulary, and discourse. However, more recent insights from \textcite{SuzukiKormos2020} suggest that the specific linguistic dimensions that primarily drive listeners' perceptions of comprehensibility remain unclear, pointing to the need for further research to disentangle these complex relationships.

\subsubsection{Perceived fluency}
As for perceived fluency, researchers have investigated the relationship between utterance fluency and perceived fluency to identify the specific temporal features that influence listeners' judgments of fluency. There are mixed findings regarding the relative importance of speed fluency (e.g., speech rate) and breakdown fluency (e.g., pause) related to perceived fluency. Some studies find that speed fluency measures correlate more strongly with perceived fluency \parencite[e.g.,][]{bosker2013}, while others show that breakdown fluency measures are more strongly associated with perceived fluency \parencite{cucchiarini2002,SuzukiKormos2020}. Taken together, the relationship between utterance fluency and perceived fluency has similar patterns as comprehensibility; it remains unclear which specific linguistic dimensions primarily drive listeners' perceptions of perceived fluency.

\subsubsection{Function words and listeners' judgments in L2 Japanese}
Previous research on the linguistic dimensions of listener judgments, such as comprehensibility, intelligibility (the actual degree of listener understanding; \citealt{MunroDerwing1999}), and accentedness (the perceived influence of the speaker's native or nonnative features on L2 speech; \citealt{DerwingMunro2015}) has focused mainly on English \parencite[e.g.,][]{crowther2018,saito2016a}. Although recent studies have begun exploring languages such as French \parencite{Bergeron2017}, Spanish \parencite{huensch2021}, and Japanese \parencite{saito2017}, it is unclear how readily findings from English transfer to typologically distinct languages like Japanese.

Japanese grammar relies heavily on function words that mark grammatical relationships \parencite{makino1986}. These include case particles (e.g., ga, o, ni) to specify who does what to whom, conjunctive particles (e.g., kedo, temo) to link clauses, adverbial particles (e.g., made, dake) to highlight or limit information, and auxiliary verbs (e.g., reru, tai) to add layers of modality and aspect \parencite[see][for a review]{nakagawa2020}. Because these elements play a central role, their misuse or omission can obscure meaning or disrupt speech flow, potentially affecting comprehensibility and perceived fluency. Although \textcite{saito2017} have begun examining linguistic correlates of comprehensibility in L2 Japanese, little is known about the specific roles of function words in listeners' judgments.

\subsection{Task effects}

\subsubsection{Task complexity manipulation}
\textcite{Robinson2005}'s cognition hypothesis and \textcite{Skehan2009}'s limited attentional capacity hypothesis both suggest that task demands affect L2 performance. Robinson suggests that cognitively complex tasks promote L2 learners' accuracy and complexity but lower fluency, as higher cognitive demands facilitate communicative interaction (i.e., opportunities for meaning negotiation and feedback) and linguistic uptake. In contrast, Skehan argues that complex tasks limit learners' ability to focus simultaneously on language aspects, especially accuracy and complexity. To investigate this issue further, researchers have often utilized Robinson's framework of task classification \parencite{Robinson2005} to explore the relationship between listeners' judgments and the linguistic dimensions of tasks of varying complexity. This framework suggests that the resource-directing dimensions of tasks, such as ±here and now, ±few elements, and ±intentional reasoning, lead to varying cognitive demands.

Recently, two studies have investigated the impact of task complexity on the relationship between listeners' judgments and a broad range of linguistic dimensions \parencite{Bergeron2017,crowther2018}. \textcite{Bergeron2017} compared a less complex picture narrative task (+few elements, +prior knowledge, +here and now, +planning) with a more complex interview task (−few elements, −planning, −here and now). Following \textcite{Robinson2005}'s framework, the authors classified the interview task as placing greater cognitive demands on L2 learners. In the less complex task, accentedness was related to pronunciation and fluency, while comprehensibility was also related to lexis, grammar, and discourse. In the more complex task, both accentedness and comprehensibility were linked to a broader, similar set of linguistic dimensions. This ``task-specific'' effect underscores that the relative importance of different linguistic dimensions to listeners' judgments is not fixed; rather, it varies with the cognitive demands imposed by the task.

\textcite{crowther2018} utilized three tasks of varying complexity: a Picture Task (+few elements, +spatial knowledge, +here/now, −causal reasoning, −intentional reasoning, −perspective-taking), an IELTS Task (+few elements, +spatial knowledge, −here/now, −causal reasoning, −intentional reasoning, −perspective-taking), and a TOEFL Task (−few elements, −spatial knowledge, −here/now, +causal reasoning, +intentional reasoning, +perspective-taking). Following \textcite{Robinson2005}'s framework, the authors rated TOEFL task as the most complex, then IELTS, and finally the picture task. L2 speakers' perceived difficulty supported this classification, with the TOEFL task rated as more difficult than both the picture and IELTS tasks, thus validating the task complexity manipulation. No significant difference in perceived difficulty was found between the latter two. Their results showed that comprehensibility was consistently related to pronunciation and lexicogrammar. In contrast, only lexical appropriateness and grammatical accuracy correlated with accentedness in the picture task, but all five lexicogrammar measures were associated with accentedness in the more complex IELTS and TOEFL tasks. Thus, the linguistic correlates of comprehensibility remained stable, whereas the linguistic correlates of accentedness changed with task complexity. This finding diverges from \textcite{Bergeron2017}, who reported that linguistic correlates of comprehensibility were also task-specific.

\subsubsection{Validation of task complexity manipulation}
A crucial question in task complexity research is whether task manipulations genuinely capture the cognitive demands they aim to induce \parencite{revesz2014}. Recent studies investigating the linguistic correlates of listeners' judgments across different tasks have attempted to validate these manipulations using learners' subjective measures \parencite[e.g.,][]{crowther2018}. Similarly, \textcite{fullana2024} manipulated the ±reasoning dimension in a ``dinner table'' task, and L2 learners reported that the complex version was more difficult and mentally demanding, supporting the validity of the complexity manipulation.

Beyond self-ratings, \textcite{revesz2014} recommends validation methods such as (i) time estimations, (ii) dual-task methodology, and (iii) psychophysiological measures \parencite[see][for a review of these methods]{sasayama2016}. Subsequent studies have directly addressed these methodological concerns. \textcite{sasayama2016} compared four monologic tasks of varying complexity (one, two, four, or nine elements) using dual-task methodology, time estimation, and self-ratings. The results demonstrated that speakers perceived the one-element task as the least complex and the nine-element task as the most complex, suggesting that cognitive measures capture only large differences in task complexity. Similarly, \textcite{Lee2019} examined three tasks: the Map Task (zero, two, four elements), the Seating Arrangement Task (four, six, eight elements), and the Car Accident Task (one, three, ten elements). Cognitive demand was measured using the same methods as \textcite{sasayama2016} (dual-task methodology, time estimation, and self-ratings), suggesting that element numbers significantly affect cognitive demands, though inconsistently across task types.

Additionally, \textcite{sasayama2016} cautioned that subjective ratings often yield larger effect sizes than objective measures, possibly because speakers' perceptions can be swayed by superficial task features. Conversely, objective measures, such as reaction times, can fail to capture subtler aspects of a task, particularly if participants become disengaged from the primary task or if the secondary measure is inadequately calibrated. Together, these issues highlight the importance of using both types of measurements for a more holistic view of task complexity. Therefore, the present study employed both subjective ratings and a dual-task method to validate the manipulation of task complexity.

\subsection{Research questions}
Our research aimed to investigate how linguistic dimensions shape comprehensibility and perceived fluency in tasks of varying complexity. The present study is guided by the following three research questions:
\begin{enumerate}
    \item To what extent are comprehensibility and perceived fluency ratings related and distinct when native listeners evaluate L2 speech produced by learners of Japanese across tasks of varying complexity?
    \item How do the linguistic dimensions correlate with comprehensibility across these tasks?
    \item How do the linguistic dimensions correlate with perceived fluency across these tasks?
\end{enumerate}



\subsection{Statistical analysis}
Table \ref{tab:descriptive_stats_study2} presents the descriptive statistics for the judgment scores and linguistic measures. All statistical analyses were conducted in R (version 4.3.0; \citealt{rcore2023}). To investigate the relationship between comprehensibility and perceived fluency, we conducted separate analyses for the low- and high-complexity tasks. Normality checks using the Shapiro-Wilk test indicated non-normality only for comprehensibility scores in the high-complexity task. Consequently, parametric tests (t-test, Pearson correlation) were used for analyses within the low-complexity task. For the high-complexity task, the corresponding non-parametric tests (Wilcoxon signed-rank test, Spearman correlation) were employed. Effect sizes for these analyses were calculated and interpreted following \textcite{plonsky2014}'s guidelines.


\begin{table}[ht]
\centering
\caption{Descriptive statistics of comprehensibility, perceived fluency, and linguistic dimensions}
\label{tab:descriptive_stats_study2}
\begin{tabular}{lcccccccc}
\hline
& \multicolumn{4}{c}{Low-complexity task} & \multicolumn{4}{c}{High-complexity task} \\
\cline{2-5} \cline{6-9}
& & & \multicolumn{2}{c}{95\% CI} & & & \multicolumn{2}{c}{95\% CI} \\
\cline{4-5} \cline{8-9}
Measures & \textit{M} & \textit{SD} & \textit{Lower} & \textit{Upper} & \textit{M} & \textit{SD} & \textit{Lower} & \textit{Upper} \\
\hline
\textbf{Listeners' judgments} & & & & & & & & \\
Comprehensibility & 5.73 & 1.60 & 5.54 & 5.92 & 5.51 & 1.65 & 5.31 & 5.70 \\
Perceived fluency & 5.84 & 1.60 & 5.65 & 6.03 & 5.59 & 1.67 & 5.40 & 5.79 \\
\textbf{Syntactic complexity} & & & & & & & & \\
Clause per AS-unit & 1.44 & 0.39 & 1.31 & 1.58 & 1.33 & 0.22 & 1.26 & 1.41 \\
Case particles per AS-unit & 1.77 & 0.70 & 1.53 & 2.00 & 1.51 & 0.76 & 1.24 & 1.77 \\
Conjunctive particles per AS-unit & 0.94 & 0.64 & 0.72 & 1.16 & 0.64 & 0.46 & 0.48 & 0.80 \\
Adverbial particles per AS-unit & 1.17 & 0.50 & 1.00 & 1.34 & 0.90 & 0.40 & 0.77 & 1.04 \\
Auxiliary verbs per AS-unit & 1.29 & 0.79 & 1.02 & 1.56 & 1.45 & 0.65 & 1.22 & 1.67 \\
\textbf{Lexical complexity} & & & & & & & & \\
Lexical density & 46.33 & 3.65 & 45.08 & 47.58 & 48.54 & 4.39 & 47.03 & 50.04 \\
Different content words per AS-unit & 4.65 & 1.50 & 4.13 & 5.16 & 4.13 & 1.01 & 3.79 & 4.48 \\
Number of content words per AS-unit & 6.35 & 1.83 & 5.72 & 6.98 & 5.62 & 1.40 & 5.14 & 6.10 \\
\textbf{Accuracy} & & & & & & & & \\
Error rate & 0.91 & 0.59 & 0.71 & 1.11 & 0.68 & 0.53 & 0.50 & 0.86 \\
\textbf{Speed fluency} & & & & & & & & \\
Speech rate & 203.97 & 47.62 & 187.62 & 220.33 & 185.37 & 49.68 & 168.31 & 202.44 \\
Articulation rate & 320.93 & 50.76 & 303.49 & 338.36 & 300.64 & 47.52 & 284.32 & 316.97 \\
Mean length of run & 7.41 & 1.62 & 6.85 & 7.96 & 7.35 & 1.64 & 6.78 & 7.91 \\
\textbf{Breakdown fluency} & & & & & & & & \\
Mid-clause pause duration & 0.69 & 0.22 & 0.61 & 0.76 & 0.73 & 0.23 & 0.65 & 0.81 \\
Mid-clause pause ratio & 0.20 & 0.08 & 0.17 & 0.23 & 0.19 & 0.08 & 0.16 & 0.22 \\
Final-clause pause duration & 0.89 & 0.30 & 0.78 & 0.99 & 1.02 & 0.40 & 0.88 & 1.16 \\
Final-clause pause ratio & 0.13 & 0.04 & 0.12 & 0.15 & 0.16 & 0.06 & 0.14 & 0.18 \\
Filled pause ratio & 0.13 & 0.06 & 0.11 & 0.15 & 0.14 & 0.06 & 0.13 & 0.16 \\
\textbf{Repair fluency} & & & & & & & & \\
Fysfluency rate & 1.78 & 1.45 & 1.28 & 2.27 & 1.87 & 1.49 & 1.36 & 2.38 \\
\hline
\end{tabular}
\end{table}


We used linear mixed-effects models to investigate how linguistic dimensions shape comprehensibility and perceived fluency. Separate models were run for low- and high-complexity tasks to identify the potentially distinct sets of linguistic dimensions driving comprehensibility and perceived fluency within each task condition. Due to multicollinearity, number of content words per AS-unit, articulation rate, and mean length of run were excluded as predictors. For each outcome (comprehensibility or perceived fluency) under low- and high-complexity, we fit a full model using the lme4 package \parencite{bates2015} with all remaining linguistic dimensions as fixed effects, random intercepts for speaker and rater to control for baseline differences across speakers and overall rating tendencies among raters, and covariates for rater familiarity with Chinese accented speech and teaching experience. Model selection proceeded in two stages: (1) the dredge function from the MuMIn package \parencite{barton2024} generated all predictor combinations; (2) backward stepwise selection using step minimized Akaike Information Criterion (AIC). We chose the most parsimonious model by the lowest Schwarz's Bayesian Information Criterion (BIC). To test each predictor's unique contribution, a drop-one analysis was performed by systematically removing each fixed effect one at a time from the final model and comparing the resulting reduced model to the full model using likelihood ratio tests, changes in AIC, and marginal $R^2_m$. Fixed effects whose removal significantly increased AIC and lowered $R^2_m$ were considered essential. We then performed power analyses using the simr package \parencite{green2016} by running 1000 simulations for each remaining fixed effect. Power was defined as the percentage of simulations achieving significance ($p < .05$), reflecting the probability of detecting the effect given our model and sample size. Effect sizes were interpreted following \textcite{plonsky2018}'s guidelines.


\section{Results}

\subsection{Validation of cognitive task complexity manipulation}

To confirm the successful manipulation of task complexity, we analyzed speakers' self-reported perceived difficulty and mental effort, and their performance on the secondary task (LISAS). Table \ref{tab:task_validation_study2} presents the descriptive statistics for these measures. A Wilcoxon signed-rank test showed that speakers rated the high-complexity task as significantly more difficult ($M = 7.22$, $SD = 1.57$) than the low-complexity task ($M = 5.44$, $SD = 1.52$), $p < .001$, with a mean difference of 1.78, 95\% CI [1.33, 2.22], and a large effect size ($r = 0.94$). They also reported that the high-complexity task required significantly more mental effort ($M = 7.31$, $SD = 1.33$) than the low-complexity task ($M = 5.81$, $SD = 1.37$), $p < .001$, with a mean difference of 1.50, 95\% CI [1.10, 1.90], and a similarly large effect size ($r = 0.93$). A paired-samples \textit{t}-test indicated that LISAS scores were higher in the high-complexity task ($M = 2189.57$, $SD = 501.66$) compared to the low-complexity task ($M = 2044.54$, $SD = 457.11$), $t(35) = 3.00$, $p = .013$, with a mean difference of 145.02, 95\% CI [32.70, 257.35], the effect size was smaller than small ($d = 0.44$). These findings confirm that the high-complexity task elicited greater perceived difficulty, demanded more mental effort, and produced longer reaction times than the low-complexity task, thereby validating the intended cognitive complexity manipulation.


\begin{table}[ht]
\centering
\caption{Task difficulty, mental effort, and LISAS scores}
\label{tab:task_validation_study2}
\begin{tabular}{lcccccccc}
\hline
& \multicolumn{4}{c}{Low-complexity task} & \multicolumn{4}{c}{High-complexity task} \\
\cline{2-5} \cline{6-9}
& & & \multicolumn{2}{c}{95\% CI} & & & \multicolumn{2}{c}{95\% CI} \\
\cline{4-5} \cline{8-9}
Measures & \textit{M} & \textit{SD} & \textit{Lower} & \textit{Upper} & \textit{M} & \textit{SD} & \textit{Lower} & \textit{Upper} \\
\hline
Perceived difficulty & 5.44 & 1.52 & 4.93 & 5.96 & 7.22 & 1.57 & 6.69 & 7.75 \\
Mental Effort & 5.81 & 1.37 & 5.34 & 6.27 & 7.31 & 1.33 & 6.86 & 7.75 \\
LISAS & 2044.54 & 457.11 & 1890.00 & 2190.00 & 2189.57 & 501.66 & 2020.00 & 2359.00 \\
Accuracy Rate (\%) & 78.05 & 17.29 & 72.11 & 83.99 & 73.84 & 20.42 & 66.82 & 80.85 \\
RTCorrect & 1659.73 & 723.07 & 1565.14 & 1754.33 & 1729.32 & 734.43 & 1639.30 & 1819.33 \\
\hline
\multicolumn{9}{l}{\textit{Note.} RTCorrect = reaction time for correct responses (in milliseconds)} \\
\end{tabular}
\end{table}


\subsection{Interrater reliability}
Interrater reliability was assessed separately for each task using Cronbach's alpha and Intraclass Correlation Coefficients (ICCs) (Table \ref{tab:interrater_reliability_study2}). High Cronbach's alpha values (.87--.91) were observed, reflecting overall consistency in the ratings. The ICCs offer specific indices for single-rater and average-rater consistency and agreement, allowing for a more detailed analysis. Focusing on the most relevant ICC forms for our design (ICC3/ICC3k, reflecting consistency with the same set of raters), the moderate ICC3 values (.47--.56) suggest that the consistency of any single rater, chosen at random, was moderate. However, the excellent ICC3k values (.87--.91) demonstrate that the averaged ratings from all eight raters provided a highly consistent and reliable measure of speaker comprehensibility and perceived fluency across both tasks.

\begin{table}[ht]
\centering
\caption{Interrater reliability statistics for comprehensibility and perceived fluency}
\label{tab:interrater_reliability_study2}
\begin{tabular}{lccccccc}
\hline
& Cronbach's $\alpha$ & ICC$_1$ & ICC$_2$ & ICC$_3$ & ICC$_{1k}$ & ICC$_{2k}$ & ICC$_{3k}$ \\
\hline
\textbf{Low-complexity task} & & & & & & & \\
Comprehensibility & .88 & .38 & .39 & .47 & .83 & .84 & .88 \\
Perceived Fluency & .90 & .39 & .40 & .53 & .83 & .84 & .90 \\
\textbf{High-complexity task} & & & & & & & \\
Comprehensibility & .87 & .33 & .35 & .46 & .80 & .81 & .87 \\
Perceived Fluency & .91 & .41 & .43 & .56 & .85 & .86 & .91 \\
\hline
\multicolumn{8}{p{14cm}}{\textit{Note.} ICC$_1$ = One-way random effects (assumes different random raters per target; measures absolute agreement); ICC$_2$ = Two-way random effects (assumes same random raters per target; measures absolute agreement); ICC$_3$ = Two-way mixed effects (assumes same specific raters per target; measures consistency). Suffix \textit{k} refers to the reliability of the average rating across \textit{k} raters, while no suffix refers to single rater reliability.} \\
\end{tabular}
\end{table}



\subsection{Relationship between comprehensibility and perceived fluency}
To examine whether comprehensibility and perceived fluency are distinct constructs, we conducted correlational and mean-comparison analyses, followed by a CFA in each task. In the low-complexity task, a Pearson correlation showed a strong positive link between comprehensibility and perceived fluency ($r(33) = .94$, $p < .001$, 95\% CI [0.88, 0.97], $d = 5.51$). A paired \textit{t}-test found no significant difference between comprehensibility ($M = 5.73$, $SD = 1.60$) and perceived fluency ($M = 5.84$, $SD = 1.60$; $t(34) = -1.79$, $p = .083$, 95\% CI [$-0.24$, 0.02], $d = -0.30$). In the high-complexity task, a Spearman correlation indicated a strong relationship ($\rho(33) = .93$, $p < .001$, 95\% CI [0.86, 0.96], $d = 4.91$), and a Wilcoxon test showed no significant difference between comprehensibility ($M = 5.51$, $SD = 1.65$) and perceived fluency ($M = 5.59$, $SD = 1.67$; $V = 130.5$, $p = .099$, $d = -0.23$).


\subsection{Linguistic dimensions of comprehensibility} 
\label{study2_comprehensibility}
To investigate how linguistic dimensions shape comprehensibility across different levels of task complexity, we ran separate mixed-effects models and verified key assumptions. Shapiro-Wilk tests indicated no significant deviations from normality ($p > .05$), residual-versus-fitted plots indicated no heteroscedasticity, and all Variance Inflation Factors (VIF) stayed below 4. Random intercepts for speaker and rater had non-zero variances, suggesting no issues. These diagnostic checks suggested that the model assumptions were adequately met, and the results can be considered valid. Table \ref{tab:comprehensibility_models_study2} presents the best-fitting comprehensibility models for both tasks.

\begin{table}[ht]
\centering
\caption{Fixed effects, drop-one analysis, and power estimates for the comprehensibility models}
\label{tab:comprehensibility_models_study2}
\begin{tabular}{lcccccccc}
\hline
Fixed effects & $\beta$ & \textit{SE} & \textit{t} & \textit{p} & $\Delta R^2$ & $\Delta$AIC & LRT & LRT & Power \\
& & & & & & & $\chi^2(1)$ & \textit{p} & (\%) \\
\hline
\textbf{Low-complexity task} & & & & & & & & & \\
FinalPR & $-0.61$ & 0.16 & $-3.85$ & $<.001$ & 0.124 & $+8.67$ & 13.05 & $<.001$ & 96.40 \\
ER & $-0.51$ & 0.16 & $-3.24$ & .003 & 0.088 & $+5.50$ & 9.75 & .002 & 87.70 \\
\textbf{High-complexity task} & & & & & & & & & \\
C/AS & 0.79 & 0.21 & 3.80 & $<.001$ & 0.100 & $+8.75$ & 14.25 & $<.001$ & 94.90 \\
FinalPR & $-0.70$ & 0.20 & $-3.50$ & .002 & 0.084 & $+7.11$ & 12.41 & $<.001$ & 92.50 \\
DCW/AS & $-0.69$ & 0.25 & $-2.78$ & .010 & 0.053 & $+4.10$ & 8.34 & .004 & 76.30 \\
AuxV/AS & $-0.68$ & 0.22 & $-3.11$ & .004 & 0.067 & $+5.41$ & 10.17 & .001 & 84.20 \\
CaseP/AS & 0.45 & 0.19 & 2.33 & .027 & 0.037 & $+1.69$ & 6.08 & .014 & 62.90 \\
DysfRate & $-0.39$ & 0.15 & $-2.60$ & .014 & 0.046 & $+2.29$ & 7.41 & .007 & 71.70 \\
\hline
\multicolumn{9}{p{15cm}}{\textit{Note.} FinalPR = final-clause pause ratio. ER = error rate. C/AS = clauses per AS-unit. DCW/AS = different content words per AS-unit. AuxV/AS = auxiliary verbs per AS-unit. CaseP/AS = case particles per AS-unit. DysfRate = dysfluency rate. All continuous predictors were standardized. $\Delta R^2$ represents the change in $R^2_m$ when each predictor is added to a null model. $\Delta$AIC shows the increase in the Akaike Information Criterion if the predictor is removed from the full model. LRT $\chi^2(1)$ and \textit{p} values are from likelihood ratio tests comparing the full model versus a model without the given predictor. Power (\%) refers to observed power.} \\
\end{tabular}
\end{table}

\subsubsection{Comprehensibility model in low-complexity task}
Table \ref{tab:comprehensibility_models_study2} shows the low-complexity task's comprehensibility model with two significant fixed effects. Increased final-clause pause ratio ($\Delta R^2 = .124$) and error rate ($\Delta R^2 = .088$) reduced comprehensibility. Random effects analysis revealed significant variance in comprehensibility scores attributable to speaker (Variance $= 0.64$, $SD = 0.80$) and rater variation (Variance $= 0.43$, $SD = 0.65$), with residual variance at 1.17 ($SD = 1.08$). The model explained 16.4\% of the variance in comprehensibility through fixed effects alone ($R^2_m = .164$) and 56.2\% when random effects were included (conditional $R^2$, $R^2_c = .562$). According to \textcite{plonsky_ghanbar_2018}'s guidelines, these indicate small and large effects, respectively. Drop-one analyses showed that removing either predictor significantly worsened model fit, confirming their independent contributions (see Table \ref{tab:comprehensibility_models_study2} for details). Power analyses indicated high observed power for both final-clause pause ratio (96.40\%) and error rate (87.70\%), suggesting robust detection of their effects.

\subsubsection{Comprehensibility model in high-complexity task}
As also shown in Table \ref{tab:comprehensibility_models_study2}, the high-complexity task's comprehensibility model revealed six significant fixed effects. Increased clauses per AS-unit ($\Delta R^2 = .100$) and case particles per AS-unit ($\Delta R^2 = .037$) enhanced comprehensibility, while higher final-clause pause ratio ($\Delta R^2 = .084$), auxiliary verbs per AS-unit ($\Delta R^2 = .067$), lexical diversity (different content words per AS-unit, $\Delta R^2 = .053$), and dysfluency rate ($\Delta R^2 = .046$) reduced it. Random effects analysis revealed considerable variance in comprehensibility scores attributable to speaker (Variance $= 0.55$, $SD = 0.74$) and rater variation (Variance $= 0.65$, $SD = 0.81$), with residual variance at 1.19 ($SD = 1.09$). The model accounted for 18.9\% of variance through fixed effects ($R^2_m = .189$) and 59.7\% with random effects ($R^2_c = .597$), representing small and large effects, respectively. Drop-one analyses confirmed each effect's contribution, with power analyses showing high power for clauses per AS-unit (94.90\%), final-clause pause ratio (92.50\%), and adequate power for others (62.90\%--84.20\%).

\subsection{Linguistic dimension of perceived fluency}
Following the steps in Section \ref{study2_comprehensibility}, we fitted mixed-effects models to assess fixed effects on perceived fluency in each task. Table \ref{tab:perceived_fluency_models_study2} presents the best-fitting perceived fluency models for both tasks.


\begin{table}[ht]
\centering
\caption{Fixed effects, drop-one analysis, and power estimates for the perceived fluency models}
\label{tab:perceived_fluency_models_study2}
\begin{tabular}{lcccccccc}
\hline
Fixed effects & $\beta$ & \textit{SE} & \textit{t} & \textit{p} & $\Delta R^2$ & $\Delta$AIC & LRT & LRT & Power \\
& & & & & & & $\chi^2(1)$ & \textit{p} & (\%) \\
\hline
\textbf{Low-complexity task} & & & & & & & & & \\
FinalPR & $-0.59$ & 0.16 & $-12.16$ & $<.001$ & 0.120 & $+7.86$ & 12.16 & $<.001$ & 95.50 \\
ER & $-0.52$ & 0.16 & $-3.24$ & .003 & 0.083 & $+5.60$ & 9.81 & .002 & 87.80 \\
\textbf{High-complexity task} & & & & & & & & & \\
C/AS & 0.55 & 0.21 & 6.75 & .014 & 0.069 & $+3.01$ & 6.75 & .009 & 71.30 \\
AuxV/AS & $-0.49$ & 0.26 & 3.85 & .064 & 0.054 & $+0.70$ & 3.85 & .050 & 46.80 \\
FinalPR & $-0.45$ & 0.23 & 3.89 & .063 & 0.065 & $+0.52$ & 3.89 & .049 & 45.30 \\
\hline
\multicolumn{9}{p{14cm}}{\textit{Note.} FinalPR = final-clause pause ratio. ER = error rate. C/AS = clauses per AS-unit. AuxV/AS = auxiliary verbs per AS-unit. All continuous predictors were standardized.} \\
\end{tabular}
\end{table}


\subsubsection{Perceived fluency model in low-complexity task}
Table \ref{tab:perceived_fluency_models_study2} shows the low-complexity task's perceived fluency model with two significant negative fixed effects. Increased final-clause pause ratio ($\Delta R^2 = .120$) and error rate ($\Delta R^2 = .083$) reduced perceived fluency. Random effects analysis revealed considerable variance in fluency scores attributable to speaker (Variance $= 0.69$, $SD = 0.83$) and rater variation (Variance $= 0.63$, $SD = 0.79$), with residual variance at 0.96 ($SD = 0.98$). The model explained 15.9\% of variance through fixed effects ($R^2_m = .159$) and 64.5\% with random effects ($R^2_c = .645$), representing small and large effects, respectively. Drop-one analyses confirmed both effects' contributions, with high power for final-clause pause ratio (95.50\%) and error rate (87.80\%), indicating robust detection.

\subsubsection{Perceived fluency model in high-complexity task}
As also shown in Table \ref{tab:perceived_fluency_models_study2}, the high-complexity task's perceived fluency model revealed one significant fixed effect and two marginally significant effects. Increased clauses per AS-unit ($\Delta R^2 = .069$) enhanced perceived fluency, while final-clause pause ratio and auxiliary verbs per AS-unit suggested slight reductions. Random effects analysis revealed considerable variance in perceived fluency scores attributable to speaker (Variance $= 0.99$, $SD = 1.00$) and rater variation (Variance $= 0.67$, $SD = 0.82$), with residual variance at 0.99 ($SD = 1.00$). The model accounted for 11.2\% of variance through fixed effects ($R^2_m = .112$) and 66.9\% with random effects ($R^2_c = .669$), representing small and large effects, respectively. While removing the final-clause pause ratio and auxiliary verbs per AS-unit led to slight increases in AIC and trend-level LRT values, suggesting potential contributions, power analyses revealed low observed power for these two fixed effects (45.30\% and 46.80\%, respectively). Power for clauses per AS-unit was adequate (71.30\%).

\section{Discussion}

\subsection{RQ1: Relationship between comprehensibility and perceived fluency}
Addressing the first research question, strong positive correlations were revealed between comprehensibility and perceived fluency across both low- ($r = .94$, $p < .001$) and high-complexity tasks ($\rho = .93$, $p < .001$). This strong correlation replicates findings from prior research \parencite{SuzukiKormos2020}. However, regarding the distinctness of these judgments, our analyses revealed no significant difference between the scores in either task ($p > .05$), indicating listeners made little distinction. This finding contrasts with \textcite{SuzukiKormos2020}, who report a significant difference between the scores. This divergence might result from differing rater experience (experienced in this study vs. inexperienced in their study) or fluency assessed (narrow sense in this study vs. broad sense in theirs). Consequently, for this study's experienced raters, comprehensibility and perceived fluency were strongly related but not practically distinct, regardless of task complexity.

\subsection{RQ2: Linguistic correlates of comprehensibility across task complexity}
\label{RQ2discussion_study}
When investigating how linguistic dimensions relate to L2 Japanese comprehensibility, our findings showed that in the low-complexity task, final-clause pause ratio and error rate were significant negative fixed effects, suggesting that in less complex tasks, these salient features are primary cues listeners use to judge the ease of understanding. This aligns with previous research highlighting breakdown fluency \parencite{kang2010} and accuracy \parencite{derwing2015} as key indicators of comprehensibility.

However, the relationship between linguistic dimensions and comprehensibility became more multifaceted in the high-complexity task. Although final-clause pause ratio still negatively affected comprehensibility, clauses per AS-unit and case particles per AS-unit positively predicted it, suggesting that under higher cognitive demands, listeners rely on elaborate syntactic structures and explicit grammatical markers to accurately decode the speaker's intended message. Conversely, dysfluency rate, different content words per AS-unit, and auxiliary verbs per AS-unit emerged as negative fixed effects. These results differ from the findings from \textcite{saito2017}'s study on L2 Japanese comprehensibility, whose linear mixed-effects model identified lexical variation as a significant predictor of comprehensibility. Their correlation analysis also confirmed that greater lexical variation was positively associated with comprehensibility. One likely explanation for this discrepancy lies in the difference in task complexity: whereas \textcite{saito2017} investigated picture narrative tasks, our high-complexity task places greater cognitive demands on speakers and listeners, potentially overshadowing the benefits of lexical variation.

The shift of the linguistic correlates of comprehensibility parallels \textcite{Skehan2009}'s limited attentional capacity model, where speakers trade-off complexity, fluency, and accuracy due to limited resources. Our findings reveal a parallel effect in perception: increased task complexity shifts listeners' prioritization of linguistic dimensions when judging L2 speech. On the one hand, greater syntactic sophistication can enhance comprehensibility in high-complexity tasks; on the other, excessive verb constructions or high lexical variation may overburden listeners' processing capacities under more demanding conditions.

Another noteworthy finding that contrasts with prior research concerns the role of pause location in comprehensibility. While \textcite{SuzukiKormos2020} found mid-clause pausing to be a primary cue for comprehensibility in English, only final-clause pausing was a significant predictor in the current study. A likely explanation for this lies in Japanese's SOV structure. Because key morphological and semantic information (e.g., the main verb, tense/aspect markers, sentence-final particles, conjunctive particles) is often reserved for the end of a clause, pauses at this juncture force listeners to hold incomplete information in working memory. This delay in message resolution may hinder the accurate interpretation of the entire utterance.

\subsection{RQ3: Linguistic correlates of perceived fluency across task complexity}
Analyses of perceived fluency revealed a similar pattern to comprehensibility in the low-complexity task, with final-clause pause ratio and error rate emerging as significant negative fixed effects. Listeners in a simpler communicative environment appear to judge perceived fluency by the degree of temporal smoothness and absence of grammatical or lexical errors. These findings align with \textcite{cucchiarini2002} and \textcite{SuzukiKormos2020}, who found that breakdown fluency and accuracy are significant predictors shaping perceived fluency in broad sense.

In the high-complexity task, the fixed effects of perceived fluency shifted. Clauses per AS-unit showed a significant positive relationship, indicating that elaborate syntactic structures can be interpreted as a hallmark of fluent speech when speakers tackle complex content. Although final-clause pause ratio and auxiliary verbs per AS-unit exhibited only trend-level negative effects, they suggest that prolonged pausing and potentially convoluted verb constructions may still detract from perceived fluency in more demanding communicative scenarios---even if listeners become somewhat more tolerant of pauses when speakers handle complex ideas. Similarly, the shift of linguistic correlates of perceived fluency also parallels \textcite{Skehan2009}'s model, reflecting the similar pattern of listener's prioritization to cognitive demands seen for comprehensibility.

These results align with \textcite{Segalowitz2010}'s multifaceted view of fluency, which holds that although temporal features remain crucial, they are also shaped by broader cognitive and linguistic sophistication that influences listeners' overall perceptions. However, like the pattern observed for comprehensibility, final-clause pauses significantly influenced perceived fluency. Unlike many studies of English, where mid-clause pauses tend to be most detrimental \parencite{suzuki2021}. As explained in Section \ref{RQ2discussion_study}, the SOV structure of Japanese concentrates crucial semantic and morphological information at the end of the clause. Therefore, disruptions at this boundary may become highly salient and detrimental to fluency for listeners. Ultimately, these findings confirm that perceived fluency is sensitive to surface-level markers of breakdown (especially at syntactic boundaries in less complex tasks) and to deeper syntactic complexity (notably in more complex tasks), reflecting a shift in fluency dimensions as communicative demands escalate.

\section{Conclusion}
This study investigated how task complexity differentially modulates the linguistic dimensions influencing comprehensibility and perceived fluency in L2 Japanese. Although listeners made little practical distinction between the judgments, their underlying linguistic correlates shifted significantly with task complexity, confirming our initial hypotheses that these correlates shift in different ways as task demands increase. These findings resonate with \textcite{skehan1997}'s limited attentional capacity model. Specifically, as task complexity increases and speakers produce more elaborate speech, listeners trade off prioritizing linguistic dimensions, as normally advantageous features may overload processing under higher demand. Crucially, the current study highlighted the unique role of function words in shaping listeners' judgments, reflecting their reliance on morphological cues to decode meaning in Japanese. Meanwhile, pauses at clause boundaries reduced fluency in an SOV language where extra breaks seem unnecessary or confusing.

The study faces several limitations. First, the secondary task may have diverted participants' attentional resources, potentially affecting speech production \parencite{fukuta2015}. Future research should consider alternative validation methods that do not interfere with the primary task. Second, analyzing only the initial 60 seconds of speech samples may not fully capture participants' oral performance; more comprehensive samples could yield richer insights. Lastly, our homogeneous participant group and the omission of some linguistic features (e.g., phonetic predictors) may limit generalizability. By addressing these issues, future studies can deepen our understanding of how cognitive demands shape L2 oral communication and ultimately inform more effective pedagogical and assessment practices.


